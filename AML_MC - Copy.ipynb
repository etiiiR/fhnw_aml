{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c914a105",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"AML Challenge 2024\"\n",
    "date: today\n",
    "author: \"Etienne Roulet, Alexander Shanmugam\"\n",
    "format: \n",
    "    html:\n",
    "        toc: true\n",
    "        number_sections: true\n",
    "        page-layout: full\n",
    "editor_options: \n",
    "    chunk_output_type: console\n",
    "bibliography: references.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be48810",
   "metadata": {},
   "source": [
    "### Todos\n",
    "\n",
    "- 14. Quantitatives Untersuchen der Unterschiede von Top-N Kunden-Listen verschiedener Modelle.\n",
    "- 17. Beschreiben des Mehrwerts des “finalen” Modelles in der Praxis.\n",
    "\n",
    "- remove underaged accounts not only junior cards\n",
    "- Markdown writing and explain what we have done\n",
    "- Clean up code: (Imports in central place, Comments, Code Refactor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eb51a8",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Die folgenden Code-Blöcke können genutzt werden, um die benötigten Abhängigkeiten zu installieren und zu importieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288df528",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:36.194435Z",
     "start_time": "2024-06-05T13:21:35.676924Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101af587",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:37.652875Z",
     "start_time": "2024-06-05T13:21:36.196324Z"
    }
   },
   "outputs": [],
   "source": [
    "from itables import show\n",
    "from itables import init_notebook_mode\n",
    "\n",
    "init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a0ebb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:39.850424Z",
     "start_time": "2024-06-05T13:21:37.654519Z"
    }
   },
   "outputs": [],
   "source": [
    "# Laden der eingesetzten Libraries\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "from IPython.display import display\n",
    "from itables import init_notebook_mode\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    make_scorer,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    fbeta_score,\n",
    "    cohen_kappa_score,\n",
    "    matthews_corrcoef,\n",
    ")\n",
    "import lime.lime_tabular\n",
    "import shap\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c08d54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:39.860045Z",
     "start_time": "2024-06-05T13:21:39.856230Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# set theme ggplot for plots\n",
    "plt.style.use(\"ggplot\")\n",
    "# set display options\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b48ba2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:39.874326Z",
     "start_time": "2024-06-05T13:21:39.863633Z"
    }
   },
   "outputs": [],
   "source": [
    "# Funktion zur Bestimmung des Geschlechts und Berechnung des Geburtstags\n",
    "def parse_details(birth_number):\n",
    "    birth_number_str = str(\n",
    "        birth_number\n",
    "    )  # Konvertiere birth_number zu einem String, falls notwendig\n",
    "    year_prefix = \"19\"\n",
    "    month = int(birth_number_str[2:4])\n",
    "    gender = \"female\" if month > 12 else \"male\"\n",
    "    if gender == \"female\":\n",
    "        month -= 50\n",
    "    year = int(year_prefix + birth_number_str[:2])\n",
    "    day = int(birth_number_str[4:6])\n",
    "    birth_day = datetime(year, month, day)\n",
    "    return gender, birth_day\n",
    "\n",
    "\n",
    "# Berechnung des Alters basierend auf einem Basisjahr\n",
    "def calculate_age(birth_date, base_date=datetime(1999, 12, 31)):\n",
    "    return (\n",
    "        base_date.year\n",
    "        - birth_date.year\n",
    "        - ((base_date.month, base_date.day) < (birth_date.month, birth_date.day))\n",
    "    )\n",
    "\n",
    "\n",
    "# Regression metrics\n",
    "def regression_results(y_true, y_pred):\n",
    "    print(\n",
    "        \"explained_variance: \",\n",
    "        round(metrics.explained_variance_score(y_true, y_pred), 4),\n",
    "    )\n",
    "    print(\n",
    "        \"mean_squared_log_error: \",\n",
    "        round(metrics.mean_squared_log_error(y_true, y_pred), 4),\n",
    "    )\n",
    "    print(\"r2: \", round(metrics.r2_score(y_true, y_pred), 4))\n",
    "    print(\"MAE: \", round(metrics.mean_absolute_error(y_true, y_pred), 4))\n",
    "    print(\"MSE: \", round(metrics.mean_squared_error(y_true, y_pred), 4))\n",
    "    print(\"RMSE: \", round(np.sqrt(metrics.mean_squared_error(y_true, y_pred)), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b01973",
   "metadata": {},
   "source": [
    "# Aufgabenstellung\n",
    "Inhalt der hier bearbeiteten und dokumentierten Mini-Challenge für das Modul «aml - Angewandtes Machine Learning» der FHNW ist die Entwicklung und Evaluierung von Aﬀinitätsmodellen für personalisierte Kreditkarten-Werbekampagnen im Auftrag einer Bank. Das Ziel der Authoren ist es also, mithilfe von Kunden- und Transaktionsdaten präzise Modelle zu erstellen, die die Wahrscheinlichkeit des Kreditkartenkaufs einer bestimmten Person vorhersagen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060d4e26",
   "metadata": {},
   "source": [
    "# Laden der zur Verfügung gestellten Daten\n",
    "[//]: # (-.- .tabset)\n",
    "\n",
    "Zur Verfügung gestellt wurden 8 csv-Dateien von welchen die Beschreibung der erfassten Variablen unter dem folgenden Link eingesehen werden können: [PKDD'99 Discovery Challenge - Guide to the Financial Data Set](https://sorry.vse.cz/~berka/challenge/PAST/index.html). Nachfolgend werden diese csv-Dateien eingelesen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90835c02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:40.771057Z",
     "start_time": "2024-06-05T13:21:39.876207Z"
    }
   },
   "outputs": [],
   "source": [
    "account = pd.read_csv(\"./data/account.csv\", sep=\";\", dtype={\"date\": \"str\"})\n",
    "card = pd.read_csv(\"./data/card.csv\", sep=\";\", dtype={\"issued\": \"str\"})\n",
    "client = pd.read_csv(\"./data/client.csv\", sep=\";\")\n",
    "disp = pd.read_csv(\"./data/disp.csv\", sep=\";\")\n",
    "district = pd.read_csv(\"./data/district.csv\", sep=\";\")\n",
    "loan = pd.read_csv(\"./data/loan.csv\", sep=\";\", dtype={\"date\": \"str\"})\n",
    "order = pd.read_csv(\"./data/order.csv\", sep=\";\")\n",
    "trans = pd.read_csv(\"./data/trans.csv\", sep=\";\", dtype={\"date\": \"str\", \"bank\": \"str\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ccb119",
   "metadata": {},
   "source": [
    "# Datenaufbereitung & Explorative Datenanalyse\n",
    "Im folgenden Abschnitt werden die geladenen Daten separat so transformiert, dass jede Zeile einer Observation und jede Spalte einer Variable im entsprechenden Datenformat entspricht, also ins Tidy-Format gebracht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54353be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:40.775694Z",
     "start_time": "2024-06-05T13:21:40.772349Z"
    }
   },
   "outputs": [],
   "source": [
    "data_frames = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2061957",
   "metadata": {},
   "source": [
    "## Account\n",
    "[//]: # (-.- .tabset)\n",
    "Der Datensatz `accounts.csv` beinhaltet 4500 Observationen mit den folgenden Informationen über die Kontos der Bank: \n",
    "   \n",
    "- `account_id`: die Kontonummer,  \n",
    "- `district_id`: den Standort der entsprechenden Bankfiliale,  \n",
    "- `frequency`: die Frequenz der Ausstellung von Kontoauszügen (monatlich, wöchentlich, pro Transaktion) und  \n",
    "- `date`: das Erstellungsdatum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7c9a96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:40.788192Z",
     "start_time": "2024-06-05T13:21:40.776619Z"
    }
   },
   "outputs": [],
   "source": [
    "account.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996b14a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:40.794045Z",
     "start_time": "2024-06-05T13:21:40.788818Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Anzahl fehlender Werte:\", sum(account.isnull().sum()))\n",
    "print(\"Anzahl duplizierter Einträge:\", account.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac78a21",
   "metadata": {},
   "source": [
    "### Aufbereitung\n",
    "Nachfolgend wird die `date` Spalte des `account.csv`-Datensatzes in das entsprechende Datenformat geparsed und die Werte von `frequency` übersetzt und als Levels einer Kategorie definiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf94d61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:40.811910Z",
     "start_time": "2024-06-05T13:21:40.797161Z"
    }
   },
   "outputs": [],
   "source": [
    "# parse date\n",
    "account[\"date\"] = pd.to_datetime(account[\"date\"], format=\"%y%m%d\")\n",
    "# translate categories\n",
    "account[\"frequency\"] = account[\"frequency\"].replace(\n",
    "    {\n",
    "        \"POPLATEK MESICNE\": \"monthly\",\n",
    "        \"POPLATEK TYDNE\": \"weekly\",\n",
    "        \"POPLATEK PO OBRATU\": \"transactional\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# convert column frequency to categorical\n",
    "account[\"frequency\"] = account[\"frequency\"].astype(\"category\")\n",
    "\n",
    "# sample 5 random rows\n",
    "account.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9ff54e",
   "metadata": {},
   "source": [
    "### Distrikt\n",
    "Hier zu sehen ist die Verteilung der Distrikte pro Bankkonto. Ersichtlich ist, dass im Distrikt 1 mit Abstand am meisten Bankkontos geführt werden. Die darauf folgenden Distrikte bewegen sich alle im Bereich zwischen ~250 - 50 Bankkonten.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3191d039",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:41.196817Z",
     "start_time": "2024-06-05T13:21:40.812493Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the distribution of the district_ids and replace the id with it's name\n",
    "plt.figure(figsize=(15, 6))\n",
    "account[\"district_id\"].value_counts().plot(kind=\"bar\")\n",
    "plt.title(\"Verteilung der Distrikte\")\n",
    "plt.xlabel(\"Distrikt\")\n",
    "plt.ylabel(\"Anzahl\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97ca3ac",
   "metadata": {},
   "source": [
    "### Frequenz\n",
    "Auf dieser Visualisierung zu sehen ist die Klassenverteilung der Frequenz der Ausstellung der Kontoauszüge. Die allermeisten Bankkonten besitzen eine monatliche Ausstellung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae21c623",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:41.280193Z",
     "start_time": "2024-06-05T13:21:41.197786Z"
    }
   },
   "outputs": [],
   "source": [
    "# Verteilung der Frequenz visualisieren\n",
    "plt.figure(figsize=(10, 6))\n",
    "account[\"frequency\"].value_counts().plot(kind=\"bar\")\n",
    "plt.title(\"Frequenz der Kontoauszüge\")\n",
    "plt.xlabel(\"Frequenz\")\n",
    "plt.ylabel(\"Anzahl\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc11c89",
   "metadata": {},
   "source": [
    "### Datum\n",
    "Der hier dargestellte Plot zeigt die Verteilung der Kontoerstellungsdaten. Das erste Konto wurde im Jahr 1993 und das neuste im 1998 erstellt.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7268f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:41.443215Z",
     "start_time": "2024-06-05T13:21:41.281646Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot date distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(account[\"date\"], bins=20)\n",
    "plt.title(\"Verteilung der Kontoerstellungsdaten\")\n",
    "plt.xlabel(\"Datum\")\n",
    "plt.ylabel(\"Anzahl\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ed7142",
   "metadata": {},
   "source": [
    "### Korrelation & weitere Informationen\n",
    "Die Korrelation sowie weitere Informationen zu den vorhandenen Daten können aus dem [SweetViz Report](./reports/accounts.html) entnommen werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8abfe7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:41.457895Z",
     "start_time": "2024-06-05T13:21:41.455133Z"
    }
   },
   "outputs": [],
   "source": [
    "# append account data to dataframe collection\n",
    "data_frames[\"account.csv\"] = account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44572b1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:41.462876Z",
     "start_time": "2024-06-05T13:21:41.459275Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # generate sweetviz report\n",
    "# svReport_account = sv.analyze(account)\n",
    "# svReport_account.show_html(filepath=\"./reports/accounts.html\", open_browser=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29133bb8",
   "metadata": {},
   "source": [
    "## Card\n",
    "[//]: # (-.- .tabset)\n",
    "\n",
    "Der Datensatz `card.csv` beinhaltet 892 Observationen mit den folgenden Informationen über die von der Bank herausgegebenen Kreditkarten:  \n",
    "\n",
    "- `card_id`: die Kartennummer,  \n",
    "- `disp_id`: die Zuordnung zum entsprechenden Bankkonto und -inhaber (Disposition),  \n",
    "- `type`: die Art der Kreditkarte (junior, classic, gold) und  \n",
    "- `issued`: das Ausstellungsdatum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84210c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:41.475572Z",
     "start_time": "2024-06-05T13:21:41.467514Z"
    }
   },
   "outputs": [],
   "source": [
    "card.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8b332b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:41.481800Z",
     "start_time": "2024-06-05T13:21:41.476855Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Anzahl fehlender Werte:\", sum(card.isnull().sum()))\n",
    "print(\"Anzahl duplizierter Einträge:\", card.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a305b6c8",
   "metadata": {},
   "source": [
    "### Aufbereitung\n",
    "Auch bei diesem Datensatz (`card.csv`) werden zunächst die Datentypen korrigiert um anschliessend die Inhalte entsprechend beschreiben zu können"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3af350",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:41.499263Z",
     "start_time": "2024-06-05T13:21:41.487247Z"
    }
   },
   "outputs": [],
   "source": [
    "# parse date\n",
    "card[\"issued\"] = pd.to_datetime(card[\"issued\"].str[:6], format=\"%y%m%d\")\n",
    "card[\"issued\"] = card[\"issued\"].dt.to_period(\"M\")\n",
    "# convert type to categorical\n",
    "card[\"type\"] = card[\"type\"].astype(\"category\")\n",
    "\n",
    "card.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2475679a",
   "metadata": {},
   "source": [
    "### Kartentyp\n",
    "Hier dargestellt ist die Klassenverteilung der Kartentypen. Die meisten Karteninhaber besitzen eine klassische Kreditkarte, gefolgt von ~180 junior- und ~100 gold Karten.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12295382",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:41.578542Z",
     "start_time": "2024-06-05T13:21:41.500963Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot distribution of type\n",
    "plt.figure(figsize=(10, 6))\n",
    "card[\"type\"].value_counts().plot(kind=\"bar\")\n",
    "plt.title(\"Verteilung der Kartentypen\")\n",
    "plt.xlabel(\"Kartentyp\")\n",
    "plt.ylabel(\"Anzahl\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fbaf05",
   "metadata": {},
   "source": [
    "### Ausstellungsdatum\n",
    "Hier dargestellt ist die Häufigkeit von Kreditkartenausstellungen pro Monat. Erkennbar ist eine steigende Tendenz mit einem Rückgang in den Monaten Februar - April 1997.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1899ecaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:41.756597Z",
     "start_time": "2024-06-05T13:21:41.579879Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot issued date per month and year\n",
    "plt.figure(figsize=(15, 6))\n",
    "card[\"issued\"].value_counts().sort_index().plot(kind=\"bar\")\n",
    "plt.title(\"Verteilung der Ausstellungsdaten\")\n",
    "plt.xlabel(\"Datum\")\n",
    "plt.ylabel(\"Anzahl\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cbeb26",
   "metadata": {},
   "source": [
    "### Korrelation & weitere Informationen\n",
    "Die Korrelation sowie weitere Informationen zu den vorhandenen Daten können aus dem [SweetViz Report](./reports/card.html) entnommen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b4ade1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:41.760316Z",
     "start_time": "2024-06-05T13:21:41.758255Z"
    }
   },
   "outputs": [],
   "source": [
    "# append to dataframes collection\n",
    "data_frames[\"card.csv\"] = card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932774e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:41.764495Z",
     "start_time": "2024-06-05T13:21:41.761996Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # generate sweetviz report\n",
    "# svReport_card = sv.analyze(card)\n",
    "# svReport_card.show_html(filepath=\"./reports/card.html\", open_browser=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493b31ff",
   "metadata": {},
   "source": [
    "## Client\n",
    "[//]: # (-.- .tabset)\n",
    "\n",
    "Der Datensatz `client.csv` beinhaltet 5369 Observationen mit den folgenden Informationen über die Kunden der Bank:  \n",
    "\n",
    "- `client_id`: die Kundennummer,  \n",
    "- `birth_number`: eine Kombination aus Geburtsdatum und Geschlecht sowie  \n",
    "- `district_id`: die Adresse   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e95b3ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:41.772187Z",
     "start_time": "2024-06-05T13:21:41.765843Z"
    }
   },
   "outputs": [],
   "source": [
    "client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b07f61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:41.780146Z",
     "start_time": "2024-06-05T13:21:41.773837Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Anzahl fehlender Werte:\", sum(client.isnull().sum()))\n",
    "print(\"Anzahl duplizierter Einträge:\", client.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6929aa6",
   "metadata": {},
   "source": [
    "### Aufbereitung\n",
    "Die Spalte `birth_number` des `client.csv`-Datensatzes codiert 3 Features der Bankkunden: Geschlecht, Geburtsdatum und damit auch das Alter. Diese Informationen werden mithilfe der zuvor definierten Funktionen `parse_details()` und `calculate_age` extrahiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e591f7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:41.804883Z",
     "start_time": "2024-06-05T13:21:41.781836Z"
    }
   },
   "outputs": [],
   "source": [
    "# Geburtstag & Geschlecht aus birth_number extrahieren\n",
    "client[\"gender\"], client[\"birth_day\"] = zip(\n",
    "    *client[\"birth_number\"].apply(parse_details)\n",
    ")\n",
    "client[\"gender\"] = client[\"gender\"].astype(\"category\")\n",
    "# Alter berechnen\n",
    "client[\"age\"] = client[\"birth_day\"].apply(calculate_age)\n",
    "\n",
    "# Spalte birth_number entfernen\n",
    "client = client.drop(columns=[\"birth_number\"])\n",
    "\n",
    "# Sample 5 random rows\n",
    "client.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e760016",
   "metadata": {},
   "source": [
    "### Geschlecht\n",
    "Hier dargestellt ist die Verteilung des Geschlechts der Bankkunden. Das Geschlecht der erfassten Bankkunden ist fast gleichverteilt mit einem etwas kleineren Frauenanteil.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4c3e44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:41.892043Z",
     "start_time": "2024-06-05T13:21:41.806399Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot distribution of gender\n",
    "plt.figure(figsize=(10, 6))\n",
    "gender_distribution = client[\"gender\"].value_counts().plot(kind=\"bar\")\n",
    "plt.title(\"Verteilung des Geschlechts der Bankkunden\")\n",
    "plt.xlabel(\"Geschlecht\")\n",
    "plt.ylabel(\"Anzahl\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b4b611",
   "metadata": {},
   "source": [
    "### Alter\n",
    "Nachfolgend abgebildet ist die Verteilung des Alters der Bankkunden. Die jüngste erfasste Person ist 12 Jahre alt und die älteste 88. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9940e050",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:41.984747Z",
     "start_time": "2024-06-05T13:21:41.895159Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot distribution of age\n",
    "plt.figure(figsize=(10, 6))\n",
    "client[\"age\"].plot(kind=\"hist\", bins=20)\n",
    "plt.title(\"Verteilung des Alters der Bankkunden\")\n",
    "plt.xlabel(\"Alter\")\n",
    "plt.ylabel(\"Anzahl\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47985b62",
   "metadata": {},
   "source": [
    "### Korrelation & weitere Informationen\n",
    "Die Korrelation sowie weitere Informationen zu den vorhandenen Daten können aus dem [SweetViz Report](./reports/client.html) entnommen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c7ec50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:41.988784Z",
     "start_time": "2024-06-05T13:21:41.986070Z"
    }
   },
   "outputs": [],
   "source": [
    "data_frames[\"client.csv\"] = client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9039f7a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:41.991580Z",
     "start_time": "2024-06-05T13:21:41.989528Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# svReport_client = sv.analyze(client)\n",
    "# svReport_client.show_html(filepath=\"./reports/client.html\", open_browser=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a480fb9a",
   "metadata": {},
   "source": [
    "## Disp\n",
    "[//]: # (-.- .tabset)\n",
    "\n",
    "Der Datensatz `disp.csv` beinhaltet 5369 Observationen mit den folgenden Informationen über die Dispositionen der Bank:   \n",
    "\n",
    "- `disp_id`: der Identifikationsschlüssel der Disposition,  \n",
    "- `client_id`: die Kundennummer,  \n",
    "- `account_id`: die Kontonummer,  \n",
    "- `type`: die Art der Disposition (Inhaber, Benutzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5e6a15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:42.000423Z",
     "start_time": "2024-06-05T13:21:41.992871Z"
    }
   },
   "outputs": [],
   "source": [
    "disp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c8b162",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:42.006008Z",
     "start_time": "2024-06-05T13:21:42.001464Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Anzahl fehlender Werte:\", sum(disp.isnull().sum()))\n",
    "print(\"Anzahl duplizierter Einträge:\", disp.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ee7d62",
   "metadata": {},
   "source": [
    "### Aufbereitung\n",
    "Auch die Variablen des Datensatzes `disp.csv` werden in die korrekten Datentypen übertragen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4fac50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:42.012090Z",
     "start_time": "2024-06-05T13:21:42.007020Z"
    }
   },
   "outputs": [],
   "source": [
    "# Spalte type als Kategorie speichern\n",
    "disp[\"type\"] = disp[\"type\"].astype(\"category\")\n",
    "\n",
    "# random sample\n",
    "disp.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57216abc",
   "metadata": {},
   "source": [
    "### Typ der Disposition\n",
    "Hier dargestellt ist die Verteilung der Art der Dispositionen. 4500 Kunden sind Inhaber eines Kontos und 896 sind Disponenten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f1d5e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:42.081753Z",
     "start_time": "2024-06-05T13:21:42.013079Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot distribution of kind\n",
    "plt.figure(figsize=(10, 6))\n",
    "disp[\"type\"].value_counts().plot(kind=\"bar\")\n",
    "plt.title(\"Verteilung der Dispositionen\")\n",
    "plt.xlabel(\"Disposition\")\n",
    "plt.ylabel(\"Anzahl\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18dced5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:42.087469Z",
     "start_time": "2024-06-05T13:21:42.083040Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove disponents\n",
    "disp = disp[disp[\"type\"] == \"OWNER\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea3dad7",
   "metadata": {},
   "source": [
    "### Korrelation & weitere Informationen\n",
    "Die Korrelation sowie weitere Informationen zu den vorhandenen Daten können aus dem [SweetViz Report](./reports/disp.html) entnommen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4006ccbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:42.092328Z",
     "start_time": "2024-06-05T13:21:42.088710Z"
    }
   },
   "outputs": [],
   "source": [
    "data_frames[\"disp.csv\"] = disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4418021d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:42.097295Z",
     "start_time": "2024-06-05T13:21:42.094063Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# svReport_disp = sv.analyze(disp)\n",
    "# svReport_disp.show_html(filepath=\"./reports/disp.html\", open_browser=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0b8b30",
   "metadata": {},
   "source": [
    "## District\n",
    "[//]: # (-.- .tabset)\n",
    "\n",
    "Der Datensatz `district.csv` beinhaltet 77 Observationen mit den folgenden demografischen Informationen:   \n",
    "\n",
    "- `A1`: die ID des Distrikts,  \n",
    "- `A2`: der Name des Distrikts,  \n",
    "- `A3`: die Region,  \n",
    "- `A4`: die Anzahl der Einwohner,  \n",
    "- `A5`: die Anzahl der Gemeinden mit < 499 Einwohner,  \n",
    "- `A6`: die Anzahl der Gemeinden mit 500 - 1999 Einwohner,  \n",
    "- `A7`: die Anzahl der Gemeinden mit 2000 - 9999 Einwohner,  \n",
    "- `A8`: die Anzahl der Gemeinden mit >10000 Einwohner,  \n",
    "- `A9`: die Anzahl Städte,  \n",
    "- `A10`: das Verhältnis von städtischen Einwohnern,  \n",
    "- `A11`: das durchschnittliche Einkommen,  \n",
    "- `A12`: die Arbeitslosenrate vom Jahr 95,  \n",
    "- `A13`: die Arbeitslosenrate vom Jahr 96,  \n",
    "- `A14`: die Anzahl von Unternehmer pro 1000 Einwohner,  \n",
    "- `A15`: die Anzahl von begangenen Verbrechen im Jahr 95,  \n",
    "- `A16`: die Anzahl von begangenen Verbrechen im Jahr 96,   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6900b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:42.105877Z",
     "start_time": "2024-06-05T13:21:42.099429Z"
    }
   },
   "outputs": [],
   "source": [
    "district.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2cb590",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:42.113574Z",
     "start_time": "2024-06-05T13:21:42.107245Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Anzahl fehlender Werte:\", sum(district.isnull().sum()))\n",
    "print(\"Anzahl duplizierter Einträge:\", district.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdd18df",
   "metadata": {},
   "source": [
    "### Aufbereitung\n",
    "Zunächst werden die Spaltennamen in sprechendere übersetzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5892740d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:42.122410Z",
     "start_time": "2024-06-05T13:21:42.114861Z"
    }
   },
   "outputs": [],
   "source": [
    "# Spalten umbenennen\n",
    "district = district.rename(\n",
    "    columns={\n",
    "        \"A1\": \"district_id\",\n",
    "        \"A2\": \"district_name\",\n",
    "        \"A3\": \"region\",\n",
    "        \"A4\": \"num_of_habitat\",\n",
    "        \"A5\": \"num_of_small_town\",\n",
    "        \"A6\": \"num_of_medium_town\",\n",
    "        \"A7\": \"num_of_big_town\",\n",
    "        \"A8\": \"num_of_bigger_town\",\n",
    "        \"A9\": \"num_of_city\",\n",
    "        \"A10\": \"ratio_of_urban\",\n",
    "        \"A11\": \"average_salary\",\n",
    "        \"A12\": \"unemploy_rate95\",\n",
    "        \"A13\": \"unemploy_rate96\",\n",
    "        \"A14\": \"n_of_enterpren_per1000_inhabit\",\n",
    "        \"A15\": \"no_of_crimes95\",\n",
    "        \"A16\": \"no_of_crimes96\",\n",
    "    }\n",
    ")[\n",
    "    [\n",
    "        \"district_id\",\n",
    "        \"district_name\",\n",
    "        \"region\",\n",
    "        \"num_of_habitat\",\n",
    "        \"num_of_small_town\",\n",
    "        \"num_of_medium_town\",\n",
    "        \"num_of_big_town\",\n",
    "        \"num_of_bigger_town\",\n",
    "        \"num_of_city\",\n",
    "        \"ratio_of_urban\",\n",
    "        \"average_salary\",\n",
    "        \"unemploy_rate95\",\n",
    "        \"unemploy_rate96\",\n",
    "        \"n_of_enterpren_per1000_inhabit\",\n",
    "        \"no_of_crimes95\",\n",
    "        \"no_of_crimes96\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "district[\"region\"] = district[\"region\"].astype(\"category\")\n",
    "district[\"district_name\"] = district[\"district_name\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9327d35f",
   "metadata": {},
   "source": [
    "Auffällig ist, dass nebst den Spalten `A2` (dem Namen) und `A3` (der Region) die Spalten `A12` und `A15` den Datentyp `object` erhalten. Das ist, weil jeweils ein fehlender Wert vorhanden ist, welcher mit einem `?` gekennzeichnet ist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dd722d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:42.138843Z",
     "start_time": "2024-06-05T13:21:42.124003Z"
    }
   },
   "outputs": [],
   "source": [
    "# die fehlenden Werte anzeigen\n",
    "district[district.isin([\"?\"]).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280fd360",
   "metadata": {},
   "source": [
    "Wir gehen davon aus, dass es sich hier um effektiv fehlende Werte handelt und nicht um zensierte Daten, also Werte, für welche der exakte Wert fehlt, aber trotzdem Informationen vorhanden sind. In diesem Fall, wenn die Variable mit den fehlenden Werten eine hohe Korrelation mit anderen Prediktoren aufweist, bietet es sich an, KNN oder eine einfache lineare Regression für die Imputation anzuwenden. [@brancoSurveyPredictiveModeling2017] \n",
    "\n",
    "Die Korrelationsmatrix des [SweetViz Reports](./reports/district.html) zeigt, dass `unemploy_rate95` stark mit `unemploy_rate96` und `no_of_crimes95` mit `no_of_crimes96` korreliert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b1a957",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:42.156333Z",
     "start_time": "2024-06-05T13:21:42.149709Z"
    }
   },
   "outputs": [],
   "source": [
    "# die ? ersetzen mit NaN\n",
    "district = district.replace(\"?\", np.nan)\n",
    "\n",
    "# Datentyp korrigieren\n",
    "district[\"no_of_crimes95\"] = district[\"no_of_crimes95\"].astype(float)\n",
    "district[\"unemploy_rate95\"] = district[\"unemploy_rate95\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11a0176",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:42.164979Z",
     "start_time": "2024-06-05T13:21:42.158895Z"
    }
   },
   "outputs": [],
   "source": [
    "# Korrelation zwischen Arbeitslosenquote 95 und 96\n",
    "district[[\"unemploy_rate95\", \"unemploy_rate96\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f77685",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:42.172928Z",
     "start_time": "2024-06-05T13:21:42.166748Z"
    }
   },
   "outputs": [],
   "source": [
    "# Korrelation zwischen Anzahl Verbrechen 95 und 96\n",
    "district[[\"no_of_crimes95\", \"no_of_crimes96\"]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ba71a5",
   "metadata": {},
   "source": [
    "Demnach werden nachfolgend zwei lineare Regressions-Modelle trainiert, um die fehlenden Werte zu imputieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab23763e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:42.187169Z",
     "start_time": "2024-06-05T13:21:42.174656Z"
    }
   },
   "outputs": [],
   "source": [
    "# Zeilen filtern, sodass keine fehlenden Werte vorhanden sind\n",
    "district_no_na = district[district[\"unemploy_rate95\"].notnull()]\n",
    "\n",
    "# Lineares regressions Modell erstellen\n",
    "lin_reg_unemploy = LinearRegression()\n",
    "\n",
    "# Modell fitten\n",
    "lin_reg_unemploy.fit(\n",
    "    district_no_na[\"unemploy_rate96\"].values.reshape(-1, 1),\n",
    "    district_no_na[\"unemploy_rate95\"].values,\n",
    ")\n",
    "\n",
    "# Modell evaluieren\n",
    "regression_results(\n",
    "    district_no_na[\"unemploy_rate95\"],\n",
    "    lin_reg_unemploy.predict(district_no_na[\"unemploy_rate96\"].values.reshape(-1, 1)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5513ea",
   "metadata": {},
   "source": [
    "Der $R^2$ Wert von $0.9634$ versichert, damit ein stabiles Modell für die Imputation erreicht zu haben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e035bc95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:42.194314Z",
     "start_time": "2024-06-05T13:21:42.189244Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lineares regressions Modell erstellen\n",
    "lin_reg_crime = LinearRegression()\n",
    "\n",
    "# Modell fitten\n",
    "lin_reg_crime.fit(\n",
    "    district_no_na[\"no_of_crimes96\"].values.reshape(-1, 1),\n",
    "    district_no_na[\"no_of_crimes95\"].values,\n",
    ")\n",
    "\n",
    "# Modell evaluieren\n",
    "regression_results(\n",
    "    district_no_na[\"no_of_crimes95\"],\n",
    "    lin_reg_crime.predict(district_no_na[\"no_of_crimes96\"].values.reshape(-1, 1)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d719c5",
   "metadata": {},
   "source": [
    "Auch hier mit einem $R^2$ Wert von $0.9969$ gehen wir davon aus, damit ein stabiles Modell für die Imputation erreicht zu haben. Somit werden nachfolgend die beiden Modelle genutzt, um die fehlenden Werte einzufüllen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62335ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:42.200145Z",
     "start_time": "2024-06-05T13:21:42.195892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vorhersage der fehlenden Werte\n",
    "district.loc[district[\"no_of_crimes95\"].isnull(), \"no_of_crimes95\"] = (\n",
    "    lin_reg_crime.predict(\n",
    "        district[district[\"no_of_crimes95\"].isnull()][\"no_of_crimes96\"].values.reshape(\n",
    "            -1, 1\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "district.loc[district[\"unemploy_rate95\"].isnull(), \"unemploy_rate95\"] = (\n",
    "    lin_reg_unemploy.predict(\n",
    "        district[district[\"unemploy_rate95\"].isnull()][\n",
    "            \"unemploy_rate96\"\n",
    "        ].values.reshape(-1, 1)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2994eadf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:42.211282Z",
     "start_time": "2024-06-05T13:21:42.201901Z"
    }
   },
   "outputs": [],
   "source": [
    "district.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591ccf0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:42.217312Z",
     "start_time": "2024-06-05T13:21:42.212516Z"
    }
   },
   "outputs": [],
   "source": [
    "district.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547418ee",
   "metadata": {},
   "source": [
    "### EDA\n",
    "Es gibt keine Duplikate und somit 77 unterschiedliche Namen der Distrikte. Diese sind auf 8 Regionen verteilt, wobei die meisten in south Moravia und die wenigsten in Prague liegen. Der Distrikt mit den wenigsten Einwohnern zählt 42821, im Vergleich zu demjenigen mit den meisten: 1204953, wobei die nächst kleinere Ortschaft 102609 Einwohner zählt. Weitere Informationen zu den vorhandenen Daten können aus dem [SweetViz Report](./reports/district.html) entnommen werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775cbc15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:42.221202Z",
     "start_time": "2024-06-05T13:21:42.218431Z"
    }
   },
   "outputs": [],
   "source": [
    "data_frames[\"district.csv\"] = district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45c904c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:42.224614Z",
     "start_time": "2024-06-05T13:21:42.222374Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# svReport_district = sv.analyze(district)\n",
    "# svReport_district.show_html(filepath=\"./reports/district.html\", open_browser=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068f93db",
   "metadata": {},
   "source": [
    "## Loan\n",
    "[//]: # (-.- .tabset)\n",
    "\n",
    "Der Datensatz `loan.csv` beinhaltet 682 Observationen mit den folgenden Informationen über die vergebenen Darlehen der Bank:  \n",
    "\n",
    "- `loan_id`: ID des Darlehens,  \n",
    "- `account_id`: die Kontonummer,  \n",
    "- `date`: das Datum, wann das Darlehen gewährt wurde,  \n",
    "- `amount`: der Betrag,  \n",
    "- `duration`: die Dauer des Darlehens,  \n",
    "- `payments`: die höhe der monatlichen Zahlungen und  \n",
    "- `status`: der Rückzahlungsstatus (A: ausgeglichen, B: Vertrag abgelaufen aber nicht fertig bezahlt, C: laufender Vertrag und alles in Ordnung, D: laufender Vertrag und Kunde verschuldet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d25bc1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:42.232215Z",
     "start_time": "2024-06-05T13:21:42.225981Z"
    }
   },
   "outputs": [],
   "source": [
    "loan.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc29c918",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:42.237030Z",
     "start_time": "2024-06-05T13:21:42.233098Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Anzahl fehlender Werte:\", sum(loan.isnull().sum()))\n",
    "print(\"Anzahl duplizierter Einträge:\", loan.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab1ca54",
   "metadata": {},
   "source": [
    "### Aufbereitung\n",
    "Auch für den `loan.csv` Datensatz werden zunächst Datenformate korrigiert und Kategorien übersetzt. Anschliessend wird überprüft, ob ein Bankkonto mehrere Darlehen besitzt.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2145d29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:42.241789Z",
     "start_time": "2024-06-05T13:21:42.237856Z"
    }
   },
   "outputs": [],
   "source": [
    "# Datum parsen\n",
    "loan[\"date\"] = pd.to_datetime(loan[\"date\"], format=\"%y%m%d\")\n",
    "\n",
    "# Kategorien übersetzen\n",
    "loan[\"status\"] = loan[\"status\"].map(\n",
    "    {\n",
    "        \"A\": \"contract finished\",\n",
    "        \"B\": \"finished contract, loan not paid\",\n",
    "        \"C\": \"running contract\",\n",
    "        \"D\": \"client in debt\",\n",
    "    }\n",
    ")\n",
    "\n",
    "loan[\"status\"] = loan[\"status\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74945ae9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:42.709741Z",
     "start_time": "2024-06-05T13:21:42.706100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Anzahl der Darlehen pro Kontonummer berechnen\n",
    "num_of_loan_df = (\n",
    "    loan.groupby(\"account_id\")\n",
    "    .size()\n",
    "    .reset_index(name=\"num_of_loan\")\n",
    "    .sort_values(by=\"num_of_loan\", ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f6719",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:44.973312Z",
     "start_time": "2024-06-05T13:21:44.967237Z"
    }
   },
   "outputs": [],
   "source": [
    "# Überprüfen, ob jedes Konto nur ein Darlehen hat\n",
    "num_of_loan_df[\"num_of_loan\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3155a14d",
   "metadata": {},
   "source": [
    "Von allen Bankkontos, die ein Darlehen aufgenommen haben, hat jedes Konto genau ein Darlehen zugewiesen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ae1b83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:45.082283Z",
     "start_time": "2024-06-05T13:21:45.077298Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sample 5 random rows from the joined DataFrame\n",
    "display(loan.sample(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c36e2f5",
   "metadata": {},
   "source": [
    "### Ausstellungsdatum\n",
    "Nachfolgend dargestellt ist die Verteilung der Darlehensausstellungsdaten. das erste Darlehen wurde im Juli 1993 ausgestellt und das neuste im Dezember 1998. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe6c0e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:45.368225Z",
     "start_time": "2024-06-05T13:21:45.185086Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot distribution of date\n",
    "plt.figure(figsize=(15, 6))\n",
    "loan[\"date\"].dt.to_period(\"M\").value_counts().sort_index().plot(kind=\"bar\")\n",
    "plt.title(\"Verteilung der Darlehensausstellungsdaten\")\n",
    "plt.xlabel(\"Datum\")\n",
    "plt.ylabel(\"Anzahl\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3afc63",
   "metadata": {},
   "source": [
    "### Dauer\n",
    "Hier ersichtlich ist die Verteilung der Dauer der Darlehen. Sie ist fast gleichverteilt über die 5 möglichen Optionen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c144c6a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:45.464569Z",
     "start_time": "2024-06-05T13:21:45.369830Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot duration distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "loan[\"duration\"].value_counts().plot(kind=\"bar\")\n",
    "plt.title(\"Verteilung der Darlehensdauer\")\n",
    "plt.xlabel(\"Dauer\")\n",
    "plt.ylabel(\"Anzahl\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40497727",
   "metadata": {},
   "source": [
    "### Betrag\n",
    "Hier dargestellt ist die Verteilung der Darlehensbeträge. Nur wenige Darlehensbeträge sind höher als 400000 wobei die meisten um die 100000 betragen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2f4b2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:45.859438Z",
     "start_time": "2024-06-05T13:21:45.645026Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot amount\n",
    "plt.figure(figsize=(10, 6))\n",
    "loan[\"amount\"].plot(kind=\"hist\", bins=20)\n",
    "plt.title(\"Verteilung der Darlehensbeträge\")\n",
    "plt.xlabel(\"Betrag\")\n",
    "plt.ylabel(\"Anzahl\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22650d46",
   "metadata": {},
   "source": [
    "### Status\n",
    "Der nachfolgende Plot zeigt die Klassenverteilung vom Darlehensstatus. Die meisten (~400) sind laufend und ok, rund 200 sind abgeschlossen, die Kunden von ~50 Darlehen sind verschuldet und etwas weniger wurden abgeschlossen, ohne fertig abbezahlt worden zu sein.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cc0cd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:46.102915Z",
     "start_time": "2024-06-05T13:21:45.992882Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot status distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "loan[\"status\"].value_counts().plot(kind=\"bar\")\n",
    "plt.title(\"Verteilung der Darlehensstatus\")\n",
    "plt.xlabel(\"Status\")\n",
    "plt.ylabel(\"Anzahl\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad55c29",
   "metadata": {},
   "source": [
    "### Zahlungen\n",
    "Hier ersichtlich ist die Verteilung der monatlichen Zahlungen der Darlehen. Die kleinste monatliche Zahlung beträgt 304 und die höchste 9910."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f189d57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:46.306547Z",
     "start_time": "2024-06-05T13:21:46.217971Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot payments\n",
    "plt.figure(figsize=(10, 6))\n",
    "loan[\"payments\"].plot(kind=\"hist\", bins=20)\n",
    "plt.title(\"Verteilung der monatlichen Zahlungen\")\n",
    "plt.xlabel(\"Zahlungen\")\n",
    "plt.ylabel(\"Anzahl\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da279f6",
   "metadata": {},
   "source": [
    "### Korrelation & weitere Informationen\n",
    "Die Korrelation sowie weitere Informationen zu den vorhandenen Daten können aus dem [SweetViz Report](./reports/loan.html) entnommen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d359d5b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:46.613869Z",
     "start_time": "2024-06-05T13:21:46.610888Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assign the resulting DataFrame to a dictionary for storage\n",
    "data_frames[\"loan.csv\"] = loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d0c4d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:46.892601Z",
     "start_time": "2024-06-05T13:21:46.890590Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# svReport_loan = sv.analyze(loan)\n",
    "# svReport_loan.show_html(filepath=\"./reports/loan.html\", open_browser=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31630cec",
   "metadata": {},
   "source": [
    "## Order\n",
    "[//]: # (-.- .tabset)\n",
    "\n",
    "Der Datensatz `order.csv` beinhaltet 6471 Observationen mit den folgenden Informationen über die Daueraufträge eines Kontos:  \n",
    "\n",
    "- `order_id`: die Nummer des Dauerauftrags,  \n",
    "- `account_id`: die Kontonummer von welchem der Auftrag stammt,  \n",
    "- `bank_to`: die empfangende Bank,  \n",
    "- `account_to`: das empfangende Konto,  \n",
    "- `amount`: der Betrag,  \n",
    "- `k_symbol`: die Art des Auftrags (Versicherungszahlung, Haushalt, Leasing, Darlehen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342e7904",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:47.044506Z",
     "start_time": "2024-06-05T13:21:47.040052Z"
    }
   },
   "outputs": [],
   "source": [
    "order.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d51fbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:47.191337Z",
     "start_time": "2024-06-05T13:21:47.185183Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Anzahl fehlender Werte:\", sum(order.isnull().sum()))\n",
    "print(\"Anzahl duplizierter Einträge:\", order.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ef374",
   "metadata": {},
   "source": [
    "### Aufbereitung\n",
    "Auch für `order.csv` werden die Kategorien zunächst übersetzt und fehlende Werte mit der Kategorie `unknown` ersetzt. Es bestehen deutlich mehr Daueraufträge als Bankkontos, was darauf hindeutet, dass ein Bankkonto mehrere Daueraufträge eingerichtet haben kann. Zur weiteren Verarbeitung der Daten wird das Format so geändert, dass pro Konto ein `order`-Eintrag existiert.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae4ab15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:47.337712Z",
     "start_time": "2024-06-05T13:21:47.334209Z"
    }
   },
   "outputs": [],
   "source": [
    "# Kategorien übersetzen und fehlende Werte mit \"unknown\" füllen\n",
    "order[\"k_symbol\"] = (\n",
    "    order[\"k_symbol\"]\n",
    "    .map(\n",
    "        {\n",
    "            \"POJISTNE\": \"insurance_payment\",\n",
    "            \"SIPO\": \"household\",\n",
    "            \"UVER\": \"loan_payment\",\n",
    "            \"LEASING\": \"leasing\",\n",
    "        }\n",
    "    )\n",
    "    .fillna(\"unknown\")\n",
    ")\n",
    "\n",
    "order[\"k_symbol\"] = order[\"k_symbol\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fc1d18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:47.493825Z",
     "start_time": "2024-06-05T13:21:47.474341Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge with 'account_id_df' to ensure all accounts are represented\n",
    "order = pd.merge(account[[\"account_id\"]], order, on=\"account_id\", how=\"left\")\n",
    "\n",
    "# After merging, fill missing values that may have been introduced\n",
    "order[\"k_symbol\"] = order[\"k_symbol\"].fillna(\"unknown\")\n",
    "order[\"amount\"] = order[\"amount\"].fillna(0)\n",
    "order[\"has_order\"] = ~order.isna().any(axis=1)\n",
    "\n",
    "orders_pivot = order.pivot_table(\n",
    "    index=\"account_id\",\n",
    "    columns=\"k_symbol\",\n",
    "    values=\"amount\",\n",
    "    aggfunc=\"sum\",\n",
    "    observed=False,\n",
    ")\n",
    "\n",
    "# Add prefix to column names\n",
    "orders_pivot.columns = orders_pivot.columns\n",
    "\n",
    "orders_pivot = orders_pivot.reset_index()\n",
    "\n",
    "# NaN to 0\n",
    "orders_pivot = orders_pivot.fillna(0)\n",
    "# Sample 5 random rows from the merged DataFrame\n",
    "orders_pivot.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991b375a",
   "metadata": {},
   "source": [
    "### Empfangende Bank\n",
    "Die Verteilung der empfangenden Banken ist ziemlich ausgeglichen, wobei in 742 Observationen diese Angabe fehlt.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe775efc",
   "metadata": {},
   "source": [
    "### Empfangendes Konto\n",
    "Auch bei den empfangenden Konten scheint es keine auffällige Konzentration bei wenigen Konten zu geben und bei 742 Observationen fehlt die Angabe ebenfalls.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7882b491",
   "metadata": {},
   "source": [
    "### Betrag\n",
    "Der Betrag bewegt sich im Bereich zwischen 0 - 14882 mit einem Mittelwert von 2943 und einem Median von 2249. Die Verteilung ist also stark rechtsschief"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e4ada0",
   "metadata": {},
   "source": [
    "### Art\n",
    "Die meisten Daueraufträge sind betreffend dem Haushalt eingerichtet worden (3502), die wenigsten für Leasing (341).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59ce097",
   "metadata": {},
   "source": [
    "### Korrelation & weitere Informationen\n",
    "Die Korrelation sowie weitere Informationen zu den vorhandenen Daten können aus dem [SweetViz Report](./reports/order.html) entnommen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00db60d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:47.621117Z",
     "start_time": "2024-06-05T13:21:47.618247Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming data_frames is a dictionary for storing DataFrames\n",
    "data_frames[\"order.csv\"] = orders_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b87822",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:47.826512Z",
     "start_time": "2024-06-05T13:21:47.823562Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# svReport_order = sv.analyze(order)\n",
    "# svReport_order.show_html(filepath=\"./reports/order.html\", open_browser=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7842cebc",
   "metadata": {},
   "source": [
    "## Trans\n",
    "[//]: # (-.- .tabset)\n",
    "\n",
    "Der Datensatz `trans.csv` beinhaltet 1056320 Observationen mit den folgenden Informationen über die Transaktionen eines Kontos:  \n",
    "\n",
    "- `trans_id`: die ID der Transaktion,  \n",
    "- `account_id`: die Kontonummer des ausführenden Kontos,  \n",
    "- `date`: das Datum,  \n",
    "- `type`: der Typ (Einzahlung, Bezug),  \n",
    "- `operation`: die Art der Transaktion (Bezug Kreditkarte, Bareinzahlung, Bezug über eine andere Bank, Bezug Bar, Überweisung)  \n",
    "- `amount`: der Betrag der Transaktion,  \n",
    "- `balance`: der Kontostand nach ausführung der Transaktion,  \n",
    "- `k_symbol`: die Klassifikation der Transaktion (Versicherungszahlung, Kontoauszug, Zinsauszahlung, Zinszahlung bei negativem Kontostand, Haushalt, Pension, Darlehensauszahlung),  \n",
    "- `bank`: die empfangende Bank und   \n",
    "- `account`: das empfangende Bankkonto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a399de3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:48.133619Z",
     "start_time": "2024-06-05T13:21:48.003371Z"
    }
   },
   "outputs": [],
   "source": [
    "trans.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3641ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:48.640014Z",
     "start_time": "2024-06-05T13:21:48.151794Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Anzahl fehlender Werte:\", sum(trans.isnull().sum()))\n",
    "print(\"Anzahl duplizierter Einträge:\", trans.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d2813a",
   "metadata": {},
   "source": [
    "### Aufbereitung\n",
    "Die Kategorien für `type`, `operation` und `k_symbol` wurden übersetzt und die Datentypen korrigiert.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d238d3a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:49.426944Z",
     "start_time": "2024-06-05T13:21:48.641005Z"
    }
   },
   "outputs": [],
   "source": [
    "trans[\"date\"] = pd.to_datetime(trans[\"date\"], format=\"%y%m%d\")\n",
    "\n",
    "# Update 'type' column\n",
    "trans[\"type\"] = trans[\"type\"].replace(\n",
    "    {\"PRIJEM\": \"credit\", \"VYDAJ\": \"withdrawal\", \"VYBER\": \"withdrawal\"}\n",
    ")\n",
    "trans[\"type\"] = trans[\"type\"].astype(\"category\")\n",
    "\n",
    "# Update 'operation' column\n",
    "trans[\"operation\"] = trans[\"operation\"].replace(\n",
    "    {\n",
    "        \"VYBER KARTOU\": \"credit card withdrawal\",\n",
    "        \"VKLAD\": \"credit in cash\",\n",
    "        \"PREVOD Z UCTU\": \"collection from another bank\",\n",
    "        \"VYBER\": \"cash withdrawal\",\n",
    "        \"PREVOD NA UCET\": \"remittance to another bank\",\n",
    "    }\n",
    ")\n",
    "trans[\"operation\"] = trans[\"operation\"].astype(\"category\")\n",
    "\n",
    "# Update 'k_symbol' column\n",
    "trans[\"k_symbol\"] = trans[\"k_symbol\"].replace(\n",
    "    {\n",
    "        \"POJISTNE\": \"insurance payment\",\n",
    "        \"SLUZBY\": \"statement payment\",\n",
    "        \"UROK\": \"interest credited\",\n",
    "        \"SANKC. UROK\": \"sanction interest if negative balance\",\n",
    "        \"SIPO\": \"household payment\",\n",
    "        \"DUCHOD\": \"pension credited\",\n",
    "        \"UVER\": \"loan payment\",\n",
    "    }\n",
    ")\n",
    "trans[\"k_symbol\"] = trans[\"k_symbol\"].astype(\"category\")\n",
    "\n",
    "# negate the amount if type is credit\n",
    "trans.loc[trans[\"type\"] == \"withdrawal\", \"amount\"] = trans.loc[\n",
    "    trans[\"type\"] == \"withdrawal\", \"amount\"\n",
    "] * (-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b70d378",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:49.459311Z",
     "start_time": "2024-06-05T13:21:49.428662Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sample 5 random rows from the DataFrame\n",
    "trans.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612ac1ef",
   "metadata": {},
   "source": [
    "### Zeitliche Entwicklung eines Kontos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b1840c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:49.673641Z",
     "start_time": "2024-06-05T13:21:49.459975Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot Zeitliche Entwicklung des Konto-Saldos für die Konto nummer 19\n",
    "account_19 = trans[trans[\"account_id\"] == 19].copy()  # Create a copy of the DataFrame\n",
    "# Ensure the date column is in datetime format\n",
    "account_19[\"date\"] = pd.to_datetime(account_19[\"date\"])\n",
    "\n",
    "# Sort the values by date\n",
    "account_19 = account_19.sort_values(\"date\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(account_19[\"date\"], account_19[\"balance\"])\n",
    "plt.title(\"Time evolution of balance for account number 19\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Balance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711ec71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:49.942136Z",
     "start_time": "2024-06-05T13:21:49.679543Z"
    }
   },
   "outputs": [],
   "source": [
    "# zoom the year 1995 of the plot\n",
    "account_19_1995 = account_19[account_19[\"date\"].dt.year == 1995]\n",
    "# plot it\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(account_19_1995[\"date\"], account_19_1995[\"balance\"])\n",
    "plt.title(\"Time evolution of balance for account number 19 in 1995\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Balance\")\n",
    "plt.show()\n",
    "\n",
    "# Wee see that there is a steep line in 1995-10 so there are two transactions, this we have to clean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2d7dbe",
   "metadata": {},
   "source": [
    "### Korrelation & weitere Informationen\n",
    "Die Korrelation sowie weitere Informationen zu den vorhandenen Daten können aus dem [SweetViz Report](./reports/trans.html) entnommen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bbd39f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:49.962398Z",
     "start_time": "2024-06-05T13:21:49.943612Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assign to a dictionary if needed (similar to list assignment in R)\n",
    "data_frames[\"trans.csv\"] = trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796903b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:49.971765Z",
     "start_time": "2024-06-05T13:21:49.963881Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# svReport_trans = sv.analyze(trans)\n",
    "# svReport_trans.show_html(filepath=\"./reports/trans.html\", open_browser=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5d6d0e",
   "metadata": {},
   "source": [
    "# Kombinieren der Daten zu einem Modellierungsdatensatz\n",
    "Im nachfolgenden Abschnitt werden die Daten zu statischen (Kunden-) Daten und transaktionellen (Bankdienstleistungs-) Daten kombiniert um diese anschliessend zu einem Datensatz für die Modellierung zusammenzufügen.  \n",
    " \n",
    "## Stammdaten\n",
    "Die aufbereiteten Stammdaten aus den Dateien \n",
    "\n",
    "- `disp.csv`  \n",
    "- `account.csv`  \n",
    "- `client.csv`  \n",
    "- `card.csv`  \n",
    "- `loan.csv`  \n",
    "- `order.csv`  \n",
    "- `districts.csv`  \n",
    "\n",
    "werden nachfolgend zu einem Datensatz kombiniert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402a6370",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:49.992628Z",
     "start_time": "2024-06-05T13:21:49.973253Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge dataframes\n",
    "static_data = (\n",
    "    data_frames[\"disp.csv\"]\n",
    "    .merge(data_frames[\"account.csv\"], on=\"account_id\", validate=\"1:1\", how=\"left\")\n",
    "    .merge(\n",
    "        data_frames[\"client.csv\"],\n",
    "        on=\"client_id\",\n",
    "        validate=\"1:1\",\n",
    "        suffixes=(\"_account\", \"_client\"),\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .merge(\n",
    "        data_frames[\"card.csv\"],\n",
    "        on=\"disp_id\",\n",
    "        validate=\"1:1\",\n",
    "        suffixes=(\"_disp\", \"_card\"),\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .merge(\n",
    "        data_frames[\"loan.csv\"],\n",
    "        on=\"account_id\",\n",
    "        suffixes=(\"_account\", \"_loan\"),\n",
    "        validate=\"1:1\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .merge(data_frames[\"order.csv\"], on=\"account_id\", validate=\"1:1\", how=\"left\")\n",
    "    .merge(\n",
    "        data_frames[\"district.csv\"].add_suffix(\"_account\"),\n",
    "        left_on=\"district_id_account\",\n",
    "        right_on=\"district_id_account\",\n",
    "        validate=\"m:1\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .merge(\n",
    "        data_frames[\"district.csv\"].add_suffix(\"_client\"),\n",
    "        left_on=\"district_id_client\",\n",
    "        right_on=\"district_id_client\",\n",
    "        validate=\"m:1\",\n",
    "        how=\"left\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ce23c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:49.996143Z",
     "start_time": "2024-06-05T13:21:49.993495Z"
    }
   },
   "outputs": [],
   "source": [
    "static_data[\"has_card\"] = ~static_data[\"card_id\"].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c068ed0b81a8c23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_data[\"type_card\"] = static_data[\"type_card\"].cat.add_categories([\"none\"])\n",
    "static_data.loc[static_data[\"card_id\"].isna(), \"type_card\"] = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741285453c47f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get static_data status categories\n",
    "static_data[\"status\"] = static_data[\"status\"].cat.add_categories([\"none\"])\n",
    "static_data.loc[static_data[\"status\"].isna(), \"status\"] = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f05e3eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:50.009650Z",
     "start_time": "2024-06-05T13:21:50.002124Z"
    }
   },
   "outputs": [],
   "source": [
    "static_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d15db59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:50.259882Z",
     "start_time": "2024-06-05T13:21:50.251757Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Anzahl duplizierter Einträge:\", static_data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de17f9dd-6c6e-4e23-8dcd-71ebc1e5b6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna for payments, duration, amount\n",
    "static_data[\"payments\"] = static_data[\"payments\"].fillna(0)\n",
    "static_data[\"duration\"] = static_data[\"duration\"].fillna(0)\n",
    "static_data[\"amount\"] = static_data[\"amount\"].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d280286ee286e192",
   "metadata": {},
   "source": [
    "Damit wird ein Datensatz mit 4500 individuellen Kunden und 56 Spalten erzeugt. 892 dieser Kunden besitzen eine Kreditkarte und 682 haben einen Kredit aufgenommen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66093287",
   "metadata": {},
   "source": [
    "## Entfernen der Junior Karteninhaber\n",
    "Kunden im jugendlichen Alter sind speziell interessante Kunden für eine Bank, da diese grundsätzlich noch keine bis wenige Bankdienstleistungen beziehen und somit flexibel sind. Es ist deshalb sehr vorteilhaft für ein Unternehmen diese für sich zu gewinnen, weshalb viele Banken für solche Kunden ganz spezifische Prozesse definieren. Das in dieser Aufgabenstellung gewünschte Modell würde in so einem Prozess nicht eingesetzt werden, weshalb die jugendlichen Kunden nachfolgend aus dem Datensatz entfernt werden.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b985b909",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:50.696829Z",
     "start_time": "2024-06-05T13:21:50.687804Z"
    }
   },
   "outputs": [],
   "source": [
    "num_accounts_before = len(static_data)\n",
    "# # Filter rows where 'card_type' does not contain 'junior' (case insensitive)\n",
    "static_data = static_data[\n",
    "    ~static_data[\"type_card\"].str.contains(\"junior\", case=False, na=False)\n",
    "]\n",
    "num_accounts_after = len(static_data)\n",
    "num_junior_cards = num_accounts_before - num_accounts_after\n",
    "print(f\"Number of junior cards removed: {num_junior_cards}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52f1bcf98a75ad2",
   "metadata": {},
   "source": [
    "Durch diese Entfernung wurden 145 Kunden entfernt. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b75b589",
   "metadata": {},
   "source": [
    "## Bewegungsdaten\n",
    "Um einen Datensatz zu erhalten, bei welchem jede Zeile eine Observation repräsentiert müssen die Transaktionen pro Kunde entsprechend aufgerollt werden. Das bedeutet, ein vordefiniertes Zeitfenster vor dem zu modellierenden Event zu definieren und die darin enthaltenen Daten in einer Zeile zu aggregieren. Das gesuchte Zeitfenster beinhaltet bestenfalls saisonale Gegebenheiten und stets einen Lag-Zeitraum, der die Verzögerung der Kaufentscheidung und Ausführung des Auftrags aufzeichnen soll. \n",
    "Hier wird ein Rollup-Fenster inklusive Lag von 13 Monaten eingesetzt. \n",
    "\n",
    "### Käufer\n",
    "Für Kunden, die bereits eine Kreditkarte besitzen, ist es unkompliziert, das Rollup-Fenster zu identifizieren. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aa9e03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:50.924224Z",
     "start_time": "2024-06-05T13:21:50.910019Z"
    }
   },
   "outputs": [],
   "source": [
    "# select all transactions from trans from date 1995-03-16 and account_id 150\n",
    "trans[(trans[\"date\"] == \"1995-03-16\") & (trans[\"account_id\"] == 150)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bb8fb4a9219505",
   "metadata": {},
   "source": [
    "Aus dieser Tabelle ersichtlich ist, dass für den Kunden 150 zum Datum der Eröffnung des Kontos mehrere Transaktionen vorhanden sind und dass wenn die Beträge von dem Tag aufsummiert werden, der korrekte Kontostand resultiert (1900 + 900 = 2800). Deshalb wird nachfolgend davon ausgegangen, dass die Aufsummierung der Transaktionsbeträge zum korrekten Kontostand führt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4156c87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:52.450983Z",
     "start_time": "2024-06-05T13:21:51.093741Z"
    }
   },
   "outputs": [],
   "source": [
    "# sort dataframe trans by account_id and date\n",
    "first_row_per_account = trans.groupby(\"account_id\")\n",
    "\n",
    "# select rows where amount == balance\n",
    "first_row_per_account = first_row_per_account.apply(\n",
    "    lambda x: x[x[\"amount\"] == x[\"balance\"]].iloc[0], include_groups=False\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13e99f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:52.495520Z",
     "start_time": "2024-06-05T13:21:52.451932Z"
    }
   },
   "outputs": [],
   "source": [
    "# show that there's one row per unique account_id in trans\n",
    "first_row_per_account[\"account_id\"].nunique() == trans[\"account_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc25c51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:21:52.502324Z",
     "start_time": "2024-06-05T13:21:52.496263Z"
    }
   },
   "outputs": [],
   "source": [
    "first_row_per_account.query(\"amount != balance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15caa9b6452fd2f6",
   "metadata": {},
   "source": [
    "Mit dem obigen Code wird zudem sichergestellt, dass diese Gegebenheit für alle Kunden gilt. Nachfolgend werden die Transaktionen aggregiert, sodass die Spalten  \n",
    "\n",
    "- `volume`: das Volumen, also die Summe der Ein- und Ausgaben auf dem Konto,  \n",
    "- `credit`: die Summe der Einnahmen,  \n",
    "- `withdrawal`: die Summe der Ausgaben,  \n",
    "- `n_transactions`: die Anzahl der getätigten Transaktionen und  \n",
    "- `balance`: der Kontostand\n",
    "\n",
    "pro Monat entstehen. Dieser Datensatz wird dann mithilfe der nachfolgend definierten Funktion `rollup_credit_card` aufgerollt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70495d87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:22:10.765128Z",
     "start_time": "2024-06-05T13:21:52.503740Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract year and month from date to a new column 'year_month'\n",
    "trans[\"year_month\"] = trans[\"date\"].dt.to_period(\"M\")\n",
    "\n",
    "# Group by 'account_id' and 'month', and calculate the sum of 'amount', 'credit', 'withdrawal' and 'n_transactions'\n",
    "transactions_monthly = (\n",
    "    trans.groupby([\"account_id\", \"year_month\"])\n",
    "    .agg(\n",
    "        volume=(\"amount\", \"sum\"),\n",
    "        credit=(\"amount\", lambda x: x[x > 0].sum()),\n",
    "        withdrawal=(\"amount\", lambda x: x[x < 0].sum()),\n",
    "        n_transactions=(\"amount\", \"count\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# # Fill missing months for each account\n",
    "# transactions_monthly = transactions_monthly.set_index(['year_month', 'account_id']).unstack(\n",
    "#     fill_value=0).stack(future_stack=True).reset_index()\n",
    "\n",
    "# Calculate cumulative sum of 'volume' for each account\n",
    "transactions_monthly[\"balance\"] = transactions_monthly.groupby(\"account_id\")[\n",
    "    \"volume\"\n",
    "].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54a15010de145a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:22:10.770667Z",
     "start_time": "2024-06-05T13:22:10.765939Z"
    }
   },
   "outputs": [],
   "source": [
    "# count unique account_ids in transactions_monthly\n",
    "transactions_monthly[\"account_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e5d218",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:22:10.774448Z",
     "start_time": "2024-06-05T13:22:10.771323Z"
    }
   },
   "outputs": [],
   "source": [
    "def rollup_credit_card(trans_monthly, account_card_issue_dates):\n",
    "    # Add issue date and calculate months since card issue\n",
    "    trans_monthly = pd.merge(trans_monthly, account_card_issue_dates, on=\"account_id\")\n",
    "\n",
    "    trans_monthly[\"months_before_card_issue\"] = [\n",
    "        (issued - year_month).n\n",
    "        for issued, year_month in zip(\n",
    "            trans_monthly[\"issued\"], trans_monthly[\"year_month\"]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # select only where months_before_card_issue > 0 and <= 13\n",
    "    trans_monthly = trans_monthly[\n",
    "        (trans_monthly[\"months_before_card_issue\"] > 0)\n",
    "        & (trans_monthly[\"months_before_card_issue\"] <= 13)\n",
    "    ]\n",
    "\n",
    "    trans_monthly = trans_monthly.groupby(\"account_id\").filter(lambda x: len(x) == 13)\n",
    "\n",
    "    # Pivot wider\n",
    "    trans_monthly = trans_monthly.pivot_table(\n",
    "        index=\"account_id\",\n",
    "        columns=\"months_before_card_issue\",\n",
    "        values=[\"volume\", \"credit\", \"withdrawal\", \"n_transactions\", \"balance\"],\n",
    "    )\n",
    "    trans_monthly.reset_index(inplace=True)\n",
    "    trans_monthly.columns = [\n",
    "        \"_\".join(str(i) for i in col) for col in trans_monthly.columns\n",
    "    ]\n",
    "    # rename account_id_ to account_id\n",
    "    trans_monthly = trans_monthly.rename(columns={\"account_id_\": \"account_id\"})\n",
    "\n",
    "    return trans_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279e42a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:22:10.845310Z",
     "start_time": "2024-06-05T13:22:10.775196Z"
    }
   },
   "outputs": [],
   "source": [
    "buyers = static_data[static_data[\"has_card\"]]\n",
    "\n",
    "transactions_rolled_up_buyers = rollup_credit_card(\n",
    "    transactions_monthly, buyers.loc[:, [\"account_id\", \"issued\"]]\n",
    ")\n",
    "\n",
    "transactions_rolled_up_buyers.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f979542d175256f",
   "metadata": {},
   "source": [
    "Und so entsteht ein Datensatz, welcher für 568 Kreditkartenkäufer die Merkmale `balance`, `credit`, `n_transactions`, `volume` und `withdrawal` für alle 13 Monate des Rollup-Fensters inklusive Lag beinhaltet. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf2c7ff",
   "metadata": {},
   "source": [
    "### Nicht-Käufer\n",
    "Um die Transaktionen der Nicht-Käufer analog zu verarbeiten, wird ein fiktives Kaufdatum benötigt, welches als Ausgangslage für die Aufrollung dient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ac8f39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:22:10.990028Z",
     "start_time": "2024-06-05T13:22:10.846017Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the issue date distribution of the buyers that where rolled up (586)\n",
    "merged_data = transactions_rolled_up_buyers.merge(\n",
    "    buyers, how=\"left\", left_on=\"account_id\", right_on=\"account_id\"\n",
    ")\n",
    "plt.figure(figsize=(10, 6))\n",
    "merged_data[\"issued\"].value_counts().sort_index().plot(kind=\"bar\")\n",
    "plt.title(\"Verteilung der Ausstellungsdaten\")\n",
    "plt.xlabel(\"Datum\")\n",
    "plt.ylabel(\"Anzahl\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f562feb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:22:11.172469Z",
     "start_time": "2024-06-05T13:22:10.991623Z"
    }
   },
   "outputs": [],
   "source": [
    "# from transactions_monthly plot the number of distinct account ids per month\n",
    "plt.figure(figsize=(10, 6))\n",
    "transactions_monthly[\"year_month\"].value_counts().sort_index().plot(kind=\"bar\")\n",
    "plt.title(\"Anzahl der Konten pro Monat\")\n",
    "plt.xlabel(\"Monat\")\n",
    "plt.ylabel(\"Anzahl\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d20d93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:22:11.179138Z",
     "start_time": "2024-06-05T13:22:11.176237Z"
    }
   },
   "outputs": [],
   "source": [
    "def rollup_non_credit(trans_monthly, non_buyers, range):\n",
    "    # set seed\n",
    "    np.random.seed(43)\n",
    "    # for each non buyer, find the date of the first transaction\n",
    "    first_transaction_dates = (\n",
    "        trans_monthly.groupby(\"account_id\")[\"year_month\"].min().reset_index()\n",
    "    )\n",
    "    first_transaction_dates.columns = [\"account_id\", \"first_transaction_date\"]\n",
    "\n",
    "    # merge the first transaction dates with the non_buyers DataFrame\n",
    "    non_buyers = non_buyers.merge(first_transaction_dates, on=\"account_id\", how=\"left\")\n",
    "\n",
    "    # randomly sample a date from the range as issue date for each non buyer making sure that the random date is after the first transaction of the non buyer\n",
    "    non_buyers[\"issued\"] = non_buyers[\"first_transaction_date\"].apply(\n",
    "        lambda x: np.random.choice(range, 1)[0]\n",
    "    )\n",
    "\n",
    "    non_buyers_rolled_up = rollup_credit_card(\n",
    "        trans_monthly, non_buyers.loc[:, [\"account_id\", \"issued\"]]\n",
    "    )\n",
    "    return non_buyers_rolled_up, non_buyers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5963f260",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:22:11.711739Z",
     "start_time": "2024-06-05T13:22:11.180604Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the list of issue dates of buyers\n",
    "issue_dates_buyers = buyers[\"issued\"].unique()\n",
    "transactions_rolled_up_non_buyers, non_buyers = rollup_non_credit(\n",
    "    transactions_monthly, static_data[~static_data[\"has_card\"]], issue_dates_buyers\n",
    ")\n",
    "transactions_rolled_up_non_buyers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc79908",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:22:11.714781Z",
     "start_time": "2024-06-05T13:22:11.712395Z"
    }
   },
   "outputs": [],
   "source": [
    "# # model top-n similarity of districts\n",
    "# def calculate_district_similarity():\n",
    "#     data = data_frames[\"district.csv\"]\n",
    "#     data = data.drop(columns=[\"region\", \"district_name\"])\n",
    "\n",
    "#     # Calculate the similarity matrix\n",
    "#     similarity_matrix = cosine_similarity(data)\n",
    "\n",
    "#     # Create a DataFrame from the similarity matrix\n",
    "#     similarity_df = pd.DataFrame(\n",
    "#         similarity_matrix, index=data.index, columns=data.index\n",
    "#     )\n",
    "\n",
    "#     # Create a mask to exclude the diagonal\n",
    "#     mask = np.eye(len(similarity_df), dtype=bool)\n",
    "\n",
    "#     # Apply the mask to the DataFrame\n",
    "#     similarity_df = similarity_df.mask(mask)\n",
    "\n",
    "#     return similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e04f45c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:22:11.717932Z",
     "start_time": "2024-06-05T13:22:11.715461Z"
    }
   },
   "outputs": [],
   "source": [
    "# def top_n_similarity(district_id, n, similarity_df):\n",
    "#     # Get the row of the similarity matrix corresponding to the district_id\n",
    "#     similarity_row = similarity_df.loc[district_id - 1]\n",
    "\n",
    "#     # Sort the values in the row in descending order and get the top n indices\n",
    "#     top_n_indices = similarity_row.sort_values(ascending=False).head(n).index\n",
    "\n",
    "#     # append district id to top_n_indices\n",
    "#     top_n_indices = top_n_indices.append(pd.Index([district_id]))\n",
    "\n",
    "#     return top_n_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ef2995",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:22:11.721282Z",
     "start_time": "2024-06-05T13:22:11.719039Z"
    }
   },
   "outputs": [],
   "source": [
    "# # find similar non-buyers\n",
    "# def find_similar_non_buyers(buyers, non_buyers):\n",
    "#     similar_non_buyers = pd.DataFrame(\n",
    "#         columns=[\"account_id\", \"issued\"]\n",
    "#     )\n",
    "#     no_similar_found = []\n",
    "#     district_similarities = calculate_district_similarity()\n",
    "\n",
    "#     for _, buyer in buyers.iterrows():\n",
    "#         # exclude already found similar_non_buyers from non_buyer's list\n",
    "#         non_buyers = non_buyers[\n",
    "#             ~non_buyers[\"account_id\"].isin(similar_non_buyers[\"account_id\"])\n",
    "#         ]\n",
    "#         # find top n similar districts\n",
    "#         top_n_similar = top_n_similarity(\n",
    "#             buyer[\"district_id_client\"], 2, district_similarities\n",
    "#         )\n",
    "\n",
    "#         # Find non-buyers with similar age, sex, and district\n",
    "#         similar = non_buyers[\n",
    "#             (non_buyers[\"age\"] >= buyer[\"age\"] - 3)\n",
    "#             & (non_buyers[\"age\"] <= buyer[\"age\"] + 3)\n",
    "#             & (non_buyers[\"gender\"] == buyer[\"gender\"])\n",
    "#             & (non_buyers[\"district_id_account\"].isin(top_n_similar))\n",
    "#         ]\n",
    "\n",
    "#         # join transaction data to similar\n",
    "#         similar = pd.merge(similar, transactions_monthly, on=\"account_id\")\n",
    "\n",
    "#         similar[\"issued\"] = buyer[\"issued\"]\n",
    "#         similar[\"months_before_card_issue\"] = [\n",
    "#             (issued - year_month).n\n",
    "#             for issued, year_month in zip(similar[\"issued\"], similar[\"year_month\"])\n",
    "#         ]\n",
    "\n",
    "#         # select only where months_before_card_issue > 0 and <= 13\n",
    "#         similar = similar[\n",
    "#             (similar[\"months_before_card_issue\"] > 0)\n",
    "#             & (similar[\"months_before_card_issue\"] <= 13)\n",
    "#         ]\n",
    "\n",
    "#         # make sure there's 13 rows per account_id and drop account_ids for which that's not true\n",
    "#         similar = similar.groupby(\"account_id\").filter(lambda x: len(x) == 13)\n",
    "\n",
    "#         # next if similar is empty\n",
    "#         if similar.empty:\n",
    "#             no_similar_found.append(buyer[\"account_id\"])\n",
    "#             continue\n",
    "\n",
    "#         # drop duplicates\n",
    "#         similar = similar.drop_duplicates(subset=[\"account_id\"])\n",
    "\n",
    "#         # Append similar similar['account_id'] and similar['nb_issued'] to similar_non_buyers\n",
    "#         similar_non_buyers = pd.concat(\n",
    "#             [similar_non_buyers, similar[[\"account_id\", \"issued\"]]]\n",
    "#         )\n",
    "\n",
    "#     return similar_non_buyers, no_similar_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d8b48b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:22:11.724001Z",
     "start_time": "2024-06-05T13:22:11.721960Z"
    }
   },
   "outputs": [],
   "source": [
    "# non_buyers = static_data[~static_data[\"has_card\"]]\n",
    "# similar_non_buyers, none_found = find_similar_non_buyers(buyers, non_buyers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7330254f24c540",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:22:11.726224Z",
     "start_time": "2024-06-05T13:22:11.724663Z"
    }
   },
   "outputs": [],
   "source": [
    "# len(none_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d1f2e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:22:11.728774Z",
     "start_time": "2024-06-05T13:22:11.727159Z"
    }
   },
   "outputs": [],
   "source": [
    "# len(non_buyers[~non_buyers[\"account_id\"].isin(similar_non_buyers[\"account_id\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38004f1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:22:11.730738Z",
     "start_time": "2024-06-05T13:22:11.729245Z"
    }
   },
   "outputs": [],
   "source": [
    "# transactions_rolled_up_non_buyers = rollup_credit_card(\n",
    "#     transactions_monthly, similar_non_buyers\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b4c40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:22:11.960337Z",
     "start_time": "2024-06-05T13:22:11.731384Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the issue date distribution of the non-buyers that where rolled up\n",
    "plt.figure(figsize=(10, 6))\n",
    "non_buyers[\"issued\"].value_counts().sort_index().plot(kind=\"bar\")\n",
    "plt.title(\"Verteilung der Ausstellungsdaten\")\n",
    "plt.xlabel(\"Datum\")\n",
    "plt.ylabel(\"Anzahl\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7e999a",
   "metadata": {},
   "source": [
    "## Zusammenfügen der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a30f3c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:22:11.964754Z",
     "start_time": "2024-06-05T13:22:11.961480Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions_rolled_up = pd.concat(\n",
    "    [transactions_rolled_up_buyers, transactions_rolled_up_non_buyers]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b686841",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:23:15.485180Z",
     "start_time": "2024-06-05T13:23:15.458584Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge transactions_rolled_up and static data\n",
    "X = pd.merge(static_data, transactions_rolled_up, on=\"account_id\")\n",
    "y = X[\"has_card\"]\n",
    "X = X.drop(\n",
    "    columns=[\n",
    "        \"has_card\",\n",
    "        \"card_id\",\n",
    "        \"issued\",\n",
    "        \"type_card\",\n",
    "        \"loan_id\",\n",
    "        \"date_loan\",\n",
    "        \"disp_id\",\n",
    "        \"client_id\",\n",
    "        \"district_id_account\",\n",
    "        \"birth_day\",\n",
    "    ]\n",
    ")\n",
    "# convert \"date_account\" to \"days active\"\n",
    "X[\"date_account\"] = (X[\"date_account\"] - X[\"date_account\"].min()).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b245f8235d4aa005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show NaNs in X\n",
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da470383e67f23c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6fadb8",
   "metadata": {},
   "source": [
    "## Remove Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb6646a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T10:36:28.605229Z",
     "start_time": "2024-06-05T10:36:28.602261Z"
    }
   },
   "outputs": [],
   "source": [
    "# print underage accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15850296",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8223fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:24:18.127835Z",
     "start_time": "2024-06-05T13:24:18.067523Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot distribution of has_card\n",
    "plt.figure(figsize=(10, 6))\n",
    "y.value_counts().plot(kind=\"bar\")\n",
    "plt.title(\"Verteilung der Kartenbesitzer\")\n",
    "plt.xlabel(\"Kartenbesitzer\")\n",
    "plt.ylabel(\"Anzahl\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9785bb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T10:36:28.775641Z",
     "start_time": "2024-06-05T10:36:28.688195Z"
    }
   },
   "outputs": [],
   "source": [
    "## plot distribution of card_type\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# X[\"type_card\"].value_counts(dropna=False).plot(kind=\"bar\")\n",
    "# plt.title(\"Verteilung der Kartentypen\")\n",
    "# plt.xlabel(\"Kartentyp\")\n",
    "# plt.ylabel(\"Anzahl\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bb0503",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T13:32:44.268996Z",
     "start_time": "2024-06-05T13:32:44.037598Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filter the data for accounts 14 and 18\n",
    "account_data = X[X[\"account_id\"].isin([14, 18])]\n",
    "# Reshape the DataFrame for easier plotting\n",
    "months = [f\"Month {i}\" for i in range(1, 14)]\n",
    "balances = [f\"balance_{i}\" for i in range(1, 14)]\n",
    "volumes = [f\"volume_{i}\" for i in range(1, 14)]\n",
    "\n",
    "# Melt the DataFrame for balances and volumes\n",
    "balance_data = account_data.melt(\n",
    "    id_vars=\"account_id\", value_vars=balances, var_name=\"Month\", value_name=\"Balance\"\n",
    ")\n",
    "volume_data = account_data.melt(\n",
    "    id_vars=\"account_id\", value_vars=volumes, var_name=\"Month\", value_name=\"Volume\"\n",
    ")\n",
    "\n",
    "# Convert 'Month' from string to integer for proper sorting\n",
    "balance_data[\"Month\"] = balance_data[\"Month\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "volume_data[\"Month\"] = volume_data[\"Month\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "\n",
    "# Sort data by account and month\n",
    "balance_data = balance_data.sort_values(by=[\"account_id\", \"Month\"])\n",
    "volume_data = volume_data.sort_values(by=[\"account_id\", \"Month\"])\n",
    "# Plotting balance data\n",
    "plt.figure(figsize=(14, 7))\n",
    "for key, grp in balance_data.groupby(\"account_id\"):\n",
    "    plt.plot(grp[\"Month\"], grp[\"Balance\"], label=f\"Account {key} Balances\")\n",
    "plt.title(\"Monthly Balances for Accounts 14 and 18\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Balance\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting volume data\n",
    "plt.figure(figsize=(14, 7))\n",
    "for key, grp in volume_data.groupby(\"account_id\"):\n",
    "    plt.plot(grp[\"Month\"], grp[\"Volume\"], label=f\"Account {key} Volumes\")\n",
    "plt.title(\"Monthly Transaction Volumes for Accounts 14 and 18\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Volume\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ef1712aadb5cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# x get dummy variables for category\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "sm = SMOTE(random_state=43)\n",
    "X_res, y_res = sm.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f90c33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from both groups to create a balanced dataset\n",
    "# counts = X[\"has_card\"].value_counts()\n",
    "\n",
    "# Determine the minimum count between True and False\n",
    "# min_count = counts.min()\n",
    "\n",
    "# true_sample = X[X[\"has_card\"] == True].sample(n=min_count, random_state=42)\n",
    "# false_sample = X[X[\"has_card\"] == False].sample(n=min_count, random_state=42)\n",
    "\n",
    "# Combine the sampled data\n",
    "# X = pd.concat([true_sample, false_sample])\n",
    "\n",
    "# Verify the counts\n",
    "# balanced_counts = X[\"has_card\"].value_counts()\n",
    "\n",
    "# print(balanced_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd7e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of has_card\n",
    "plt.figure(figsize=(10, 6))\n",
    "y_res.value_counts().plot(kind=\"bar\")\n",
    "plt.title(\"Verteilung der Kartenbesitzer\")\n",
    "plt.xlabel(\"Kartenbesitzer\")\n",
    "plt.ylabel(\"Anzahl\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eaf751",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d1a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate features\n",
    "def calculate_features(df, prefix):\n",
    "    monthly_values = df[[f\"{prefix}_{i}\" for i in range(1, 13)]]\n",
    "    # needs to be a small constant to avoid division by zero\n",
    "    epsilon = 1e-7  # small constant\n",
    "    features = {\n",
    "        f\"{prefix}_mean\": monthly_values.mean(axis=1),\n",
    "        f\"{prefix}_min\": monthly_values.min(axis=1),\n",
    "        f\"{prefix}_max\": monthly_values.max(axis=1),\n",
    "        f\"{prefix}_mad\": monthly_values.sub(monthly_values.mean(axis=1), axis=0)\n",
    "        .abs()\n",
    "        .mean(axis=1),\n",
    "        f\"{prefix}_mean_ratio_last3_first3\": (\n",
    "            monthly_values[[f\"{prefix}_{i}\" for i in range(10, 13)]].mean(axis=1)\n",
    "            / (\n",
    "                monthly_values[[f\"{prefix}_{i}\" for i in range(1, 4)]].mean(axis=1)\n",
    "                + epsilon\n",
    "            )\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    if prefix in [\"credit\", \"withdrawal\"]:\n",
    "        features[f\"{prefix}_sum\"] = monthly_values.sum(axis=1)\n",
    "    if prefix in [\"balance\", \"credit\"]:\n",
    "        features[f\"{prefix}_std\"] = monthly_values.std(axis=1)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "# List of column prefixes for required calculations\n",
    "columns_to_process = [\"balance\", \"credit\", \"n_transactions\", \"withdrawal\"]\n",
    "\n",
    "# Generating features for each prefix and merging them\n",
    "all_features = {}\n",
    "for prefix in columns_to_process:\n",
    "    all_features.update(calculate_features(X, prefix))\n",
    "\n",
    "# Creating the final dataframe with new features\n",
    "df_features = pd.DataFrame(all_features)\n",
    "\n",
    "\n",
    "X_feature_engineered = pd.concat([X_res, df_features], axis=1)\n",
    "display(X_feature_engineered.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c04f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the Variable that can lead to data leakage\n",
    "def clean_data(df):\n",
    "    # Define unnecessary columns\n",
    "    unnecessary_cols = [\n",
    "        \"disp_id\",\n",
    "        \"client_id\",\n",
    "        \"account_id\",\n",
    "        \"type_card\",\n",
    "        \"card_id\",\n",
    "        \"loan_id\",\n",
    "        \"district_id_account\",\n",
    "        \"district_id_client\",\n",
    "    ]\n",
    "    # Drop these columns if they exist in the dataframe\n",
    "    df_cleaned = df.drop(columns=[col for col in unnecessary_cols if col in df.columns])\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "X_res = clean_data(X_res)\n",
    "X_feature_engineered = clean_data(X_feature_engineered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b39eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_feature_engineered.head(5))\n",
    "display(X_res.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e94e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only print the columns with missing values\n",
    "X_res.isnull().sum()[X_res.isnull().sum() > 0]\n",
    "X_feature_engineered.isnull().sum()[X_feature_engineered.isnull().sum() > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d8ddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values with knn imputation but in dataframe\n",
    "# import KNNImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "X_res = pd.DataFrame(imputer.fit_transform(X_res), columns=X_res.columns)\n",
    "X_feature_engineered = pd.DataFrame(\n",
    "    imputer.fit_transform(X_feature_engineered), columns=X_feature_engineered.columns\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580d09c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only print the columns with missing values\n",
    "X_res.isnull().sum()[X_res.isnull().sum() > 0]\n",
    "X_feature_engineered.isnull().sum()[X_feature_engineered.isnull().sum() > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed5b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nomalize the data and standardize the data\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_res = pd.DataFrame(scaler.fit_transform(X_res), columns=X_res.columns)\n",
    "X_feature_engineered = pd.DataFrame(\n",
    "    scaler.fit_transform(X_feature_engineered), columns=X_feature_engineered.columns\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045daec1",
   "metadata": {},
   "source": [
    "# Evaluations Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2b937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'X' is your DataFrame and 'has_card' is the target variable\n",
    "# y_features = X_feature_engineered[\"has_card\"]\n",
    "\n",
    "# X_feature_engineered.drop(\"has_card\", axis=1, inplace=True)\n",
    "# we use kfold for cross validation and then the X_test and y_test are used for evaluation on never seen data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.1, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "X_train_features, X_test_features, y_train_features, y_test_features = train_test_split(\n",
    "    X_feature_engineered, y_res, test_size=0.1, random_state=42, stratify=y_res\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b6b096",
   "metadata": {},
   "source": [
    "## Drop Featrues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cb1ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c94a5e2",
   "metadata": {},
   "source": [
    "# Modeling und Model Selection\n",
    "Für die Model Selection benutzen wir einen StratifiedKFold mit 10 Folds in dem wir nur den Train split Folden, denn später brauchen wir die Test Daten für den Error Assesment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0d9421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, models, param_grid, X, y, X_test, y_test, selected_fields=None):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator with models, their parameter grids, and data.\n",
    "\n",
    "        :param models: dict of (name, model) pairs\n",
    "        :param param_grid: dict of (name, param_grid) pairs for GridSearch\n",
    "        :param X: Feature matrix\n",
    "        :param y: Target vector\n",
    "        :param selected_fields: Fields selected for training\n",
    "        \"\"\"\n",
    "        self.benchmark_results = {}\n",
    "        self.models = models\n",
    "        self.param_grid = param_grid\n",
    "        self.X = X[selected_fields]\n",
    "        self.y = y\n",
    "        self.eval_data = X_test\n",
    "        self.eval_target = y_test\n",
    "        self.fitted_models = {}\n",
    "        self.best_models = {}\n",
    "        self.cv_predictions = {}\n",
    "\n",
    "    def get_benchmark_results(self):\n",
    "        return self.benchmark_results\n",
    "\n",
    "    def fit_models(self):\n",
    "        cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        for name, model in self.models.items():\n",
    "            pipeline = self.create_pipeline(model)\n",
    "\n",
    "            # Prefix the parameters with the step name 'model'\n",
    "            grid_search_params = {}\n",
    "            for param, values in self.param_grid[name].items():\n",
    "                grid_search_params = {\n",
    "                    f\"model__{param}\": values\n",
    "                    for param, values in self.param_grid[name].items()\n",
    "                }\n",
    "\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=pipeline,\n",
    "                param_grid=grid_search_params,\n",
    "                cv=cv,\n",
    "                scoring=\"accuracy\",\n",
    "                n_jobs=-1,\n",
    "                verbose=1,\n",
    "            )\n",
    "            grid_search.fit(self.X, self.y)\n",
    "            print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "\n",
    "            best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "            metrics = {\n",
    "                \"roc_auc\": \"roc_auc\",\n",
    "                \"precision\": \"precision\",\n",
    "                \"recall\": \"recall\",\n",
    "                \"accuracy\": \"accuracy\",\n",
    "                \"f1\": make_scorer(fbeta_score, beta=1),\n",
    "                \"kappa\": make_scorer(cohen_kappa_score),\n",
    "                \"mcc\": make_scorer(matthews_corrcoef),\n",
    "            }\n",
    "\n",
    "            self.benchmark_results[name] = {}\n",
    "            all_cv_preds = np.zeros(len(self.y))\n",
    "\n",
    "            results = cross_validate(\n",
    "                best_pipeline,\n",
    "                self.X,\n",
    "                self.y,\n",
    "                cv=cv,\n",
    "                scoring=metrics,\n",
    "                return_estimator=True,\n",
    "                n_jobs=-1,\n",
    "                verbose=0,\n",
    "            )\n",
    "\n",
    "            for metric_name in metrics.keys():\n",
    "                self.benchmark_results[name][metric_name] = np.mean(\n",
    "                    results[\"test_\" + metric_name]\n",
    "                )\n",
    "                print(\n",
    "                    f\"{name}: {metric_name} = {np.mean(results['test_' + metric_name]):.2f}\"\n",
    "                )\n",
    "\n",
    "            for train_idx, test_idx in cv.split(self.X, self.y):\n",
    "                best_pipeline.fit(self.X.iloc[train_idx], self.y.iloc[train_idx])\n",
    "                all_cv_preds[test_idx] = best_pipeline.predict_proba(\n",
    "                    self.X.iloc[test_idx]\n",
    "                )[:, 1]\n",
    "\n",
    "            self.cv_predictions[name] = all_cv_preds\n",
    "            self.fitted_models[name] = best_pipeline.fit(self.X, self.y)\n",
    "\n",
    "    def evaluate_models(self):\n",
    "        if not self.fitted_models:\n",
    "            self.fit_models()\n",
    "        return self.benchmark_results\n",
    "\n",
    "    def plot_roc_curves(self):\n",
    "        if not self.fitted_models:\n",
    "            self.fit_models()\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for name in self.fitted_models:\n",
    "            y_scores = self.cv_predictions[name]\n",
    "            fpr, tpr, _ = roc_curve(self.y, y_scores)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f\"{name} (area = {roc_auc:.2f})\")\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], \"k--\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"ROC Curves\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "    def create_pipeline(self, model):\n",
    "        categorical_cols = self.X.select_dtypes(include=[\"category\", \"object\"]).columns\n",
    "        numeric_cols = self.X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "\n",
    "        numeric_transformer = Pipeline(\n",
    "            [\n",
    "                (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                (\"scaler\", StandardScaler()),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        categorical_transformer = Pipeline(\n",
    "            [\n",
    "                (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "                (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            [\n",
    "                (\"num\", numeric_transformer, numeric_cols),\n",
    "                (\"cat\", categorical_transformer, categorical_cols),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return Pipeline([(\"preprocessor\", preprocessor), (\"model\", model)])\n",
    "\n",
    "    def compare_top_n_customers(self, model_name, n=100):\n",
    "        print(f\"Comparing top {n} customers for {model_name}\")\n",
    "        model = self.fitted_models[model_name]\n",
    "        probabilities = model.predict_proba(self.X)[:, 1]\n",
    "        top_n_indices = np.argsort(probabilities)[::-1][:n]\n",
    "\n",
    "        plt.figure()\n",
    "        plt.hist(probabilities[top_n_indices], bins=20, alpha=0.75)\n",
    "        plt.title(f\"Histogram of top {n} customers' probabilities for {model_name}\")\n",
    "        plt.xlabel(\"Probability\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_confusion_matrices(self):\n",
    "        if not self.fitted_models:\n",
    "            self.fit_models()\n",
    "\n",
    "        for name, model in self.fitted_models.items():\n",
    "            plt.style.use(\"default\")\n",
    "            y_pred = model.predict(self.X)\n",
    "            cm = confusion_matrix(self.y, y_pred)\n",
    "            disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "            disp.plot(cmap=plt.cm.Blues)\n",
    "            plt.title(f\"Confusion Matrix for {name}\")\n",
    "            plt.show()\n",
    "            plt.style.use(\"ggplot\")\n",
    "\n",
    "\n",
    "class MetricsBenchmarker:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the benchmarker with models and data.\n",
    "\n",
    "        :param models: dict of (name, model) pairs\n",
    "        :param X: Feature matrix\n",
    "        :param y: Target vector\n",
    "        :param selected_fields: Fields selected for training\n",
    "        \"\"\"\n",
    "        self.benchmark_results = {}\n",
    "        self.evals = []\n",
    "\n",
    "    def add_evaluator(self, evaluator: ModelEvaluator):\n",
    "        self.evals.append(evaluator)\n",
    "\n",
    "    def set_benchmark_results(self):\n",
    "        for eval in self.evals:\n",
    "            self.benchmark_results.update(eval.get_benchmark_results())\n",
    "\n",
    "    def display_benchmark_results_table(self):\n",
    "        \"\"\"\n",
    "        Display a table of benchmark results.\n",
    "        \"\"\"\n",
    "        results_df = pd.DataFrame(self.benchmark_results).T\n",
    "        display(results_df)\n",
    "\n",
    "    def plot_benchmark_results_bar_chart(self):\n",
    "        \"\"\"\n",
    "        Plot a bar chart of benchmark results.\n",
    "        \"\"\"\n",
    "        results_df = pd.DataFrame(self.benchmark_results).T\n",
    "        results_df.plot(kind=\"bar\", figsize=(10, 6))\n",
    "        plt.title(\"Benchmark Results\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bf1a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define models and their parameter grids\n",
    "models = {\n",
    "    \"Baseline Logistic Regression\": LogisticRegression(solver=\"liblinear\"),\n",
    "}\n",
    "param_grid = {\n",
    "    \"Baseline Logistic Regression\": {\"C\": [0.01, 0.1, 1, 10]},\n",
    "}\n",
    "\n",
    "selected_fields = (\n",
    "    [\"age\"]\n",
    "    + [col for col in X_train.columns if \"gender\" in col]\n",
    "    + [col for col in X_train.columns if \"region_client\" in col]\n",
    "    + [f\"volume_{i}\" for i in range(1, 14)]\n",
    "    + [f\"balance_{i}\" for i in range(1, 14)]\n",
    ")\n",
    "\n",
    "\n",
    "evaluator_baseline = ModelEvaluator(\n",
    "    models,\n",
    "    param_grid,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    selected_fields=selected_fields,\n",
    ")\n",
    "evaluator_baseline.evaluate_models()\n",
    "evaluator_baseline.plot_roc_curves()\n",
    "evaluator_baseline.plot_confusion_matrices()\n",
    "evaluator_baseline.compare_top_n_customers(\"Baseline Logistic Regression\", n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fca738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and their parameter grids\n",
    "models = {\n",
    "    \"Logistic Regression Features\": LogisticRegression(solver=\"liblinear\"),\n",
    "}\n",
    "param_grid = {\n",
    "    \"Logistic Regression Features\": {\"C\": [0.01, 0.1, 1, 10]},\n",
    "}\n",
    "\n",
    "selected_fields = X_train_features.columns\n",
    "\n",
    "evaluator = ModelEvaluator(\n",
    "    models,\n",
    "    param_grid,\n",
    "    X_test_features,\n",
    "    y_test_features,\n",
    "    X_train_features,\n",
    "    y_train_features,\n",
    "    selected_fields=selected_fields,\n",
    ")\n",
    "evaluator.evaluate_models()\n",
    "evaluator.plot_roc_curves()\n",
    "evaluator.compare_top_n_customers(\"Logistic Regression Features\", n=100)\n",
    "\n",
    "\n",
    "# Assuming X and y are defined\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57b7b4e",
   "metadata": {},
   "source": [
    "## Overfitting because of jagged ROC curve needs Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8887ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and their parameter grids\n",
    "models = {\n",
    "    \"Logistic Regression Features added\": LogisticRegression(solver=\"liblinear\"),\n",
    "}\n",
    "param_grid = {\n",
    "    \"Logistic Regression Features added\": {\"C\": [0.001, 0.01, 0.1, 1, 10]},\n",
    "}\n",
    "\n",
    "selected_fields = X_train_features.columns\n",
    "\n",
    "# Fix the not converging models with LassoCV\n",
    "# for model_name, model in models.items():\n",
    "#    models[model_name] = Pipeline(\n",
    "#        [\n",
    "#            (\"scaler\", StandardScaler()),\n",
    "#            (\n",
    "#                \"feature_selection\",\n",
    "#                SelectFromModel(LassoCV(alphas=[0.01, 0.1, 1, 10], max_iter=10000)),\n",
    "#            ),\n",
    "#            (\"model\", model),\n",
    "#        ]\n",
    "#    )\n",
    "\n",
    "\n",
    "evaluator = ModelEvaluator(\n",
    "    models,\n",
    "    param_grid,\n",
    "    X_test_features,\n",
    "    y_test_features,\n",
    "    X_train_features,\n",
    "    y_train_features,\n",
    "    selected_fields=selected_fields,\n",
    ")\n",
    "\n",
    "\n",
    "evaluator.evaluate_models()\n",
    "evaluator.plot_roc_curves()\n",
    "evaluator.compare_top_n_customers(\"Logistic Regression Features added\", n=100)\n",
    "\n",
    "\n",
    "# Assuming X and y are defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4705b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "# Define models and their parameter grids\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(algorithm=\"SAMME\"),\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    \"Random Forest\": {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"max_depth\": [None, 5, 10],\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"learning_rate\": [0.1, 0.01, 0.001],\n",
    "    },\n",
    "    \"SVM\": {\"C\": [0.1, 1, 10], \"kernel\": [\"linear\", \"rbf\"]},\n",
    "    \"KNN\": {\"n_neighbors\": [3, 5, 7], \"weights\": [\"uniform\", \"distance\"]},\n",
    "    \"Decision Tree\": {\n",
    "        \"max_depth\": [None, 5, 10],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "    },\n",
    "    \"AdaBoost\": {},\n",
    "}\n",
    "\n",
    "\n",
    "# Fix the not converging models with LassoCV\n",
    "# for model_name, model in models.items():\n",
    "#    models[model_name] = Pipeline(\n",
    "#        [(\"feature_selection\", SelectFromModel(LassoCV(max_iter=1500))), (\"model\", model)]\n",
    "#    )\n",
    "\n",
    "selected_fields = X_train_features.columns  # add the new features of df_features\n",
    "\n",
    "evaluator_models = ModelEvaluator(\n",
    "    models,\n",
    "    param_grid,\n",
    "    X_train_features,\n",
    "    y_train_features,\n",
    "    X_test_features,\n",
    "    y_test_features,\n",
    "    selected_fields=selected_fields,\n",
    ")\n",
    "results = evaluator_models.evaluate_models()\n",
    "evaluator_models.plot_roc_curves()\n",
    "# evaluator_models.plot_confusion_matrices()\n",
    "# compare top n customers for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9709d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare top n customers for all models\n",
    "for model_name in models.keys():\n",
    "    evaluator_models.compare_top_n_customers(model_name, n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509264c3",
   "metadata": {},
   "source": [
    "## Results Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4d1afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = MetricsBenchmarker()\n",
    "benchmark.add_evaluator(evaluator_baseline)\n",
    "benchmark.add_evaluator(evaluator_models)\n",
    "benchmark.add_evaluator(evaluator)\n",
    "benchmark.set_benchmark_results()\n",
    "benchmark.display_benchmark_results_table()\n",
    "benchmark.plot_benchmark_results_bar_chart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0eec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model\n",
    "# Define weights for the metrics\n",
    "weights = {\"roc_auc\": 0.25, \"precision\": 0.25, \"recall\": 0.25, \"f1\": 0.25}\n",
    "\n",
    "# Calculate weighted scores for each model\n",
    "weighted_scores = {}\n",
    "for model, metrics in benchmark.benchmark_results.items():\n",
    "    weighted_score = sum(\n",
    "        weights[metric] * score\n",
    "        for metric, score in metrics.items()\n",
    "        if metric in weights\n",
    "    )\n",
    "    weighted_scores[model] = weighted_score\n",
    "\n",
    "# Find the best model based on weighted score\n",
    "best_model_name = max(weighted_scores, key=weighted_scores.get)\n",
    "best_model_score = weighted_scores[best_model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e987da",
   "metadata": {},
   "source": [
    "# Model Assesment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ada034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model with the test set\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    cohen_kappa_score,\n",
    "    matthews_corrcoef,\n",
    ")\n",
    "\n",
    "\n",
    "best_model = evaluator_models.fitted_models[best_model_name]\n",
    "y_pred = best_model.predict(X_test_features)\n",
    "\n",
    "print(f\"Evaluation of the best model ({best_model_name}) using X_test:\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(f\"Confusion Matrix for {best_model_name} on X_test\")\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "y_scores = best_model.predict_proba(X_test_features)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, label=f\"{best_model_name} (area = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(f\"ROC Curve for {best_model_name} on X_test\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Other Metrics\n",
    "accuracy = accuracy_score(y_test_features, y_pred)\n",
    "precision = precision_score(y_test_features, y_pred)\n",
    "recall = recall_score(y_test_features, y_pred)\n",
    "f1 = f1_score(y_test_features, y_pred)\n",
    "kappa = cohen_kappa_score(y_test_features, y_pred)\n",
    "mcc = matthews_corrcoef(y_test_features, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"Cohen Kappa: {kappa:.2f}\")\n",
    "print(f\"Matthews Correlation Coefficient: {mcc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c2b693",
   "metadata": {},
   "source": [
    "# Erklärbare Modelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28338b70",
   "metadata": {},
   "source": [
    "## Reduziere Modell für Erklärbarkeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db984f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import lime.lime_tabular\n",
    "import shap\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Ensure all data in X_train_features are numeric\n",
    "X_train_features = X_train_features.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Drop rows with any NaN values in X_train_features and align y_res\n",
    "X_train_features = X_train_features.dropna()\n",
    "y_res_aligned = y_res.loc[X_train_features.index]\n",
    "\n",
    "X_train_reduced = X_train_features.copy()\n",
    "X_test_reduced = X_test_features.copy()\n",
    "y_test_reduced = y_test_features.copy()\n",
    "y_train_reduced = y_train_features.copy()\n",
    "\n",
    "# Convert to DataFrame with feature names for consistency\n",
    "X_train_reduced = pd.DataFrame(X_train_reduced, columns=X_train_features.columns)\n",
    "X_test_reduced = pd.DataFrame(X_test_reduced, columns=X_train_features.columns)\n",
    "\n",
    "# Add a constant column for the intercept\n",
    "X_train_reduced = sm.add_constant(X_train_reduced)\n",
    "X_test_reduced = sm.add_constant(X_test_reduced)\n",
    "\n",
    "# Convert to numeric to avoid dtype issues\n",
    "X_train_reduced = X_train_reduced.apply(pd.to_numeric)\n",
    "X_test_reduced = X_test_reduced.apply(pd.to_numeric)\n",
    "\n",
    "# Apply Lasso (L1) regularization for feature selection\n",
    "lasso_model = LogisticRegressionCV(\n",
    "    cv=5, penalty=\"l1\", solver=\"liblinear\", random_state=42, Cs=np.logspace(-4, 0, 50)\n",
    ")\n",
    "lasso_model.fit(X_train_reduced, y_train_reduced)\n",
    "\n",
    "# Get the features with non-zero coefficients\n",
    "coef = pd.Series(lasso_model.coef_[0], index=X_train_reduced.columns)\n",
    "selected_features = coef[coef != 0].index.tolist()\n",
    "print(\"Selected features after Lasso:\", selected_features)\n",
    "\n",
    "lasso_selected_fields = selected_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17131b07",
   "metadata": {},
   "source": [
    "### Logistic Regression Reduziert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203671ef",
   "metadata": {},
   "source": [
    "#### Wir machen kein Model Selection mehr sondern nur ein Model Assesment für Explainable AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1e870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import lime.lime_tabular\n",
    "import shap\n",
    "\n",
    "# Step 1: Create and Train a Reduced Model\n",
    "selected_fields_reduced = lasso_selected_fields\n",
    "\n",
    "# Ensure all data in selected fields are numeric\n",
    "X_feature_engineered[selected_fields_reduced] = X_feature_engineered[\n",
    "    selected_fields_reduced\n",
    "].apply(pd.to_numeric)\n",
    "\n",
    "X_train_reduced, X_test_reduced, y_train_reduced, y_test_reduced = train_test_split(\n",
    "    X_feature_engineered[selected_fields_reduced],\n",
    "    y_res,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=y_res,\n",
    ")\n",
    "\n",
    "# Convert to DataFrame with feature names for consistency\n",
    "X_train_reduced = pd.DataFrame(X_train_reduced, columns=selected_fields_reduced)\n",
    "X_test_reduced = pd.DataFrame(X_test_reduced, columns=selected_fields_reduced)\n",
    "\n",
    "reduced_model = LogisticRegression(solver=\"liblinear\")\n",
    "reduced_model.fit(X_train_reduced, y_train_reduced)\n",
    "\n",
    "y_pred_reduced = reduced_model.predict(X_test_reduced)\n",
    "cm = confusion_matrix(y_test_reduced, y_pred_reduced)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix for Reduced Model\")\n",
    "plt.show()\n",
    "\n",
    "y_scores_reduced = reduced_model.predict_proba(X_test_reduced)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test_reduced, y_scores_reduced)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, label=f\"Reduced Model (area = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve for Reduced Model\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Step 2: Create a Wrapper Function for LIME\n",
    "def predict_proba_with_feature_names(X):\n",
    "    X_df = pd.DataFrame(X, columns=selected_fields_reduced)\n",
    "    return reduced_model.predict_proba(X_df)\n",
    "\n",
    "\n",
    "# Explain the Model with LIME\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=X_train_reduced.values,\n",
    "    feature_names=selected_fields_reduced,\n",
    "    class_names=[\"No Card\", \"Card\"],\n",
    "    mode=\"classification\",\n",
    ")\n",
    "\n",
    "for i in range(15):\n",
    "    exp = explainer.explain_instance(\n",
    "        data_row=X_test_reduced.iloc[i].values,\n",
    "        predict_fn=predict_proba_with_feature_names,\n",
    "    )\n",
    "    exp.show_in_notebook(show_table=True)\n",
    "\n",
    "# Step 3: Explain the Model with SHAP\n",
    "# Summarize the background data using shap.sample\n",
    "background_data = shap.sample(X_train_reduced, 100)\n",
    "\n",
    "explainer_shap = shap.KernelExplainer(reduced_model.predict_proba, background_data)\n",
    "shap_values = explainer_shap.shap_values(X_test_reduced)\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "# Print shapes to debug\n",
    "print(\"SHAP values shape:\", np.array(shap_values).shape)\n",
    "print(\"X_test_reduced shape:\", X_test_reduced.shape)\n",
    "\n",
    "# Verify dimensions (consider removing the extra dimension if it exists)\n",
    "instance_index = 0  # Change the instance index if needed\n",
    "positive_class_index = 1\n",
    "\n",
    "if len(shap_values.shape) > 2:  # Check for extra dimension\n",
    "    shap_values = shap_values[\n",
    "        :, :, positive_class_index\n",
    "    ]  # Select positive class values\n",
    "\n",
    "assert len(shap_values[instance_index]) == X_test_reduced.shape[1], \"Dimension mismatch\"\n",
    "\n",
    "# Use only the SHAP values for the positive class (index 1)\n",
    "shap.force_plot(\n",
    "    explainer_shap.expected_value[positive_class_index],\n",
    "    shap_values[instance_index],\n",
    "    X_test_reduced.iloc[instance_index],\n",
    ")\n",
    "shap.summary_plot(shap_values, X_test_reduced, feature_names=selected_fields_reduced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129d068c",
   "metadata": {},
   "source": [
    "## Interpretation von den Resultaten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf7e268",
   "metadata": {},
   "source": [
    "- todo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92f159f",
   "metadata": {},
   "source": [
    "# Convert Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561272d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import subprocess\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "try:\n",
    "    file_path = pathlib.Path(os.path.basename(__file__))\n",
    "except:\n",
    "    file_path = pathlib.Path(\"AML_MC.ipynb\")\n",
    "\n",
    "# Check the file extension\n",
    "if file_path.suffix == \".py\":\n",
    "    # If it's a Python script, convert it to a notebook\n",
    "    try:\n",
    "        subprocess.check_output([\"jupytext\", \"--to\", \"notebook\", str(file_path)])\n",
    "        print(\"Converted to notebook.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Conversion failed. Error message:\", e.output)\n",
    "elif file_path.suffix == \".ipynb\":\n",
    "    # If it's a notebook, convert it to a Python script with cell markers\n",
    "    try:\n",
    "        subprocess.check_output([\"jupytext\", \"--to\", \"py:percent\", str(file_path)])\n",
    "        print(\"Converted to Python script.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Conversion failed. Error message:\", e.output)\n",
    "else:\n",
    "    print(\"Unsupported file type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af820bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "try:\n",
    "    file_path = pathlib.Path(os.path.basename(__file__))\n",
    "except:\n",
    "    file_path = pathlib.Path(\"AML_MC.ipynb\")\n",
    "\n",
    "# Check the file extension\n",
    "if file_path.suffix == \".qmd\":\n",
    "    # If it's a Python script, convert it to a notebook\n",
    "    try:\n",
    "        os.system(\"quarto convert AML_MC.qmd\")\n",
    "        print(\"Converted to notebook.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Conversion failed. Error message:\", e.output)\n",
    "elif file_path.suffix == \".ipynb\":\n",
    "    # If it's a notebook, convert it to a Python script with cell markers\n",
    "    try:\n",
    "        # quatro convert ipynb to qmd\n",
    "        os.system(\"quarto convert AML_MC.ipynb\")\n",
    "        print(\"Converted to qmd.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Conversion failed. Error message:\", e.output)\n",
    "else:\n",
    "    print(\"Unsupported file type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3b0229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.system(\"quarto render AML_MC.ipynb --to html --embed-resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d834f2-eae1-4465-9987-7bb315fb387c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
